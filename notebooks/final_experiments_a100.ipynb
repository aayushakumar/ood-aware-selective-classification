{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Environment Setup & Path Configuration\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone repo if in Colab\n",
    "    REPO_URL = 'https://github.com/aayushakumar/ood-eval-toxic-classifiers.git'\n",
    "    REPO_DIR = '/content/ood-eval-toxic-classifiers'\n",
    "    \n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        print(f\"Cloning repository...\")\n",
    "        !git clone {REPO_URL} {REPO_DIR}\n",
    "    \n",
    "    os.chdir(REPO_DIR)\n",
    "    ROOT_DIR = Path(REPO_DIR)\n",
    "    \n",
    "    # Install requirements\n",
    "    !pip install -q transformers datasets scikit-learn tqdm\n",
    "    \n",
    "elif IN_KAGGLE:\n",
    "    ROOT_DIR = Path('/kaggle/working')\n",
    "else:\n",
    "    # Local / VS Code\n",
    "    ROOT_DIR = Path('.').resolve().parent\n",
    "    if not (ROOT_DIR / 'scripts').exists():\n",
    "        ROOT_DIR = Path('.').resolve()\n",
    "\n",
    "# Add scripts to path\n",
    "SCRIPTS_DIR = ROOT_DIR / 'scripts'\n",
    "if str(SCRIPTS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCRIPTS_DIR))\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT_DIR))\n",
    "\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "OUTPUT_DIR = ROOT_DIR / 'output'\n",
    "RESULTS_DIR = OUTPUT_DIR / 'results'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "MODELS_DIR = OUTPUT_DIR / 'models'\n",
    "\n",
    "for d in [OUTPUT_DIR, RESULTS_DIR, FIGURES_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Root: {ROOT_DIR}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Scripts: {SCRIPTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043293aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Import from Project Scripts\n",
    "# =============================================================================\n",
    "\n",
    "# Standard imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, average_precision_score, roc_curve, precision_recall_curve,\n",
    "    log_loss, brier_score_loss\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import from our scripts\n",
    "from run_roberta import (\n",
    "    set_seed, load_dataset, SUPPORTED_DATASETS,\n",
    "    ToxicityDataset, build_model, train_one_epoch, evaluate,\n",
    "    expected_calibration_error, fit_temperature, fit_isotonic\n",
    ")\n",
    "from run_tfidf_baselines import (\n",
    "    evaluate_sklearn_classifier, train_and_evaluate_tfidf\n",
    ")\n",
    "from ood_algorithms import (\n",
    "    MaxSoftmaxOOD, ODIN_OOD, EnergyOOD, MahalanobisOOD,\n",
    "    CORAL, MMD,\n",
    "    TemperatureScaling, IsotonicCalibration, PlattScaling\n",
    ")\n",
    "from fairness_metrics import compute_group_fairness\n",
    "\n",
    "print(\"âœ“ Imported from project scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# Device setup\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_MEM = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {GPU_NAME} ({GPU_MEM:.1f} GB)\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Experiment config\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'datasets': ['jigsaw', 'civil', 'hatexplain'],\n",
    "    \n",
    "    # RoBERTa training\n",
    "    'model_name': 'roberta-base',\n",
    "    'max_length': 128,\n",
    "    'batch_size': 64 if 'A100' in globals().get('GPU_NAME', '') else 32,\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    \n",
    "    # TF-IDF\n",
    "    'tfidf_max_features': 50000,\n",
    "    'tfidf_ngram_range': (1, 2),\n",
    "    \n",
    "    # Paths\n",
    "    'data_dir': str(DATA_DIR),\n",
    "    'save_models': True,\n",
    "}\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "print(f\"\\nConfig: {json.dumps(CONFIG, indent=2, default=str)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ccda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Load All Datasets\n",
    "# =============================================================================\n",
    "\n",
    "data = {}\n",
    "\n",
    "for name in CONFIG['datasets']:\n",
    "    print(f\"\\nLoading {name}...\")\n",
    "    data[name] = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        try:\n",
    "            df = load_dataset(name, split, CONFIG['data_dir'])\n",
    "            data[name][split] = df\n",
    "            pos_rate = df['label'].mean()\n",
    "            print(f\"  {split}: {len(df):,} samples, {pos_rate:.1%} positive\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"  {split}: Not found\")\n",
    "\n",
    "print(\"\\nâœ“ Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Results Tracker (Lightweight)\n",
    "# =============================================================================\n",
    "\n",
    "class Results:\n",
    "    \"\"\"Simple results storage.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.classification = []\n",
    "        self.ood = []\n",
    "        self.calibration = []\n",
    "        self.fairness = []\n",
    "        self.predictions = {}\n",
    "    \n",
    "    def add(self, category, **kwargs):\n",
    "        getattr(self, category).append(kwargs)\n",
    "    \n",
    "    def get_df(self, category):\n",
    "        return pd.DataFrame(getattr(self, category))\n",
    "    \n",
    "    def save(self, path):\n",
    "        data = {\n",
    "            'classification': self.classification,\n",
    "            'ood': self.ood,\n",
    "            'calibration': self.calibration,\n",
    "            'fairness': self.fairness,\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2, default=float)\n",
    "\n",
    "results = Results()\n",
    "print(\"âœ“ Results tracker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0de62",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: TF-IDF Baseline Experiments\n",
    "\n",
    "Using `train_and_evaluate_tfidf` from `scripts/run_tfidf_baselines.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb44fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: TF-IDF Baselines - Train & Cross-Domain Eval\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TF-IDF BASELINE EXPERIMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tfidf_results = {}\n",
    "\n",
    "for source in CONFIG['datasets']:\n",
    "    if source not in data or 'train' not in data[source]:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*40}\\nTraining on {source.upper()}\\n{'='*40}\")\n",
    "    \n",
    "    # Use existing function from run_tfidf_baselines.py\n",
    "    for model_type in ['logreg', 'svm']:\n",
    "        print(f\"\\n  TF-IDF + {model_type.upper()}:\")\n",
    "        \n",
    "        # Vectorize\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=CONFIG['tfidf_max_features'],\n",
    "            ngram_range=CONFIG['tfidf_ngram_range'],\n",
    "            min_df=5, sublinear_tf=True\n",
    "        )\n",
    "        X_train = vectorizer.fit_transform(data[source]['train']['text'].fillna(''))\n",
    "        y_train = data[source]['train']['label'].values\n",
    "        \n",
    "        # Train\n",
    "        if model_type == 'logreg':\n",
    "            model = LogisticRegression(max_iter=1000, class_weight='balanced', n_jobs=-1)\n",
    "        else:\n",
    "            model = LinearSVC(max_iter=5000, dual=False, class_weight='balanced')\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on all test sets\n",
    "        for target in CONFIG['datasets']:\n",
    "            if target not in data or 'test' not in data[target]:\n",
    "                continue\n",
    "            \n",
    "            X_test = vectorizer.transform(data[target]['test']['text'].fillna(''))\n",
    "            y_test = data[target]['test']['label'].values\n",
    "            \n",
    "            # Use existing evaluation function\n",
    "            metrics = evaluate_sklearn_classifier(X_test, y_test, model, model_type)\n",
    "            \n",
    "            # Store\n",
    "            eval_type = 'in-domain' if source == target else 'cross-domain'\n",
    "            results.add('classification',\n",
    "                model=f'TF-IDF-{model_type.upper()}', source=source, target=target,\n",
    "                eval_type=eval_type, **metrics\n",
    "            )\n",
    "            \n",
    "            marker = \"â˜…\" if source == target else \"â†’\"\n",
    "            print(f\"    {marker} {target}: F1={metrics['f1']:.4f}, AUROC={metrics.get('auroc', 0):.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        if CONFIG['save_models']:\n",
    "            key = f\"tfidf_{model_type}_{source}\"\n",
    "            tfidf_results[key] = {'vectorizer': vectorizer, 'model': model}\n",
    "\n",
    "print(\"\\nâœ“ TF-IDF experiments complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489c258",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: RoBERTa Training & Cross-Domain Evaluation\n",
    "\n",
    "Using `ToxicityDataset`, `build_model`, `train_one_epoch`, `evaluate` from `scripts/run_roberta.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: RoBERTa Training\n",
    "# =============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RoBERTa TRAINING & CROSS-DOMAIN EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "roberta_models = {}\n",
    "training_histories = {}\n",
    "eval_results = {}  # Store logits/features for OOD detection\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "for source in CONFIG['datasets']:\n",
    "    if source not in data or 'train' not in data[source]:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nTraining RoBERTa on {source.upper()}\\n{'='*50}\")\n",
    "    \n",
    "    train_df = data[source]['train']\n",
    "    val_df = data[source]['val']\n",
    "    \n",
    "    # Create datasets using imported class\n",
    "    train_dataset = ToxicityDataset(\n",
    "        train_df['text'].tolist(), train_df['label'].tolist(),\n",
    "        tokenizer, CONFIG['max_length']\n",
    "    )\n",
    "    val_dataset = ToxicityDataset(\n",
    "        val_df['text'].tolist(), val_df['label'].tolist(),\n",
    "        tokenizer, CONFIG['max_length']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Build model using imported function\n",
    "    model = build_model(CONFIG['model_name'], num_labels=2).to(DEVICE)\n",
    "    \n",
    "    # Optimizer & scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=0.01)\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(total_steps * CONFIG['warmup_ratio']),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "    best_f1, best_state = 0, None\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        # Train using imported function (signature: model, dataloader, optimizer, scheduler, device)\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, DEVICE)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validate using imported function (signature: model, dataloader, device, return_probs)\n",
    "        val_metrics = evaluate(model, val_loader, DEVICE, return_probs=True)\n",
    "        history['val_loss'].append(val_metrics.get('nll', 0))\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{CONFIG['epochs']}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Val F1={val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    # Load best model\n",
    "    if best_state:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "    \n",
    "    roberta_models[source] = model\n",
    "    training_histories[source] = history\n",
    "    print(f\"  âœ“ Best Val F1: {best_f1:.4f}\")\n",
    "    \n",
    "    # Cross-domain evaluation\n",
    "    print(f\"\\n  Cross-domain evaluation:\")\n",
    "    for target in CONFIG['datasets']:\n",
    "        if target not in data or 'test' not in data[target]:\n",
    "            continue\n",
    "        \n",
    "        test_df = data[target]['test']\n",
    "        test_dataset = ToxicityDataset(\n",
    "            test_df['text'].tolist(), test_df['label'].tolist(),\n",
    "            tokenizer, CONFIG['max_length']\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Evaluate and get logits (use return_probs=True to get logits and probs)\n",
    "        metrics = evaluate(model, test_loader, DEVICE, return_probs=True)\n",
    "        \n",
    "        eval_key = f\"{source}_to_{target}\"\n",
    "        eval_results[eval_key] = {\n",
    "            'logits': metrics.get('logits'),\n",
    "            'labels': metrics.get('labels'),\n",
    "            'probs': metrics.get('pos_probs'),  # Use pos_probs (1D array for positive class)\n",
    "        }\n",
    "        \n",
    "        eval_type = 'in-domain' if source == target else 'cross-domain'\n",
    "        results.add('classification',\n",
    "            model='RoBERTa', source=source, target=target,\n",
    "            eval_type=eval_type, \n",
    "            accuracy=metrics['accuracy'], f1=metrics['f1'],\n",
    "            auroc=metrics.get('auroc', 0), ece=metrics.get('ece', 0)\n",
    "        )\n",
    "        \n",
    "        marker = \"â˜…\" if source == target else \"â†’\"\n",
    "        print(f\"    {marker} {target}: F1={metrics['f1']:.4f}, AUROC={metrics.get('auroc', 0):.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    if CONFIG['save_models']:\n",
    "        torch.save(model.state_dict(), MODELS_DIR / f'roberta_{source}.pt')\n",
    "    \n",
    "    # Clear cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nâœ“ RoBERTa training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7eed6",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: OOD Detection & Calibration\n",
    "\n",
    "Using classes from `scripts/ood_algorithms.py`:\n",
    "- OOD: `MaxSoftmaxOOD`, `ODIN_OOD`, `EnergyOOD`, `MahalanobisOOD`\n",
    "- Domain Shift: `CORAL`, `MMD`\n",
    "- Calibration: `TemperatureScaling`, `IsotonicCalibration`, `PlattScaling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: OOD Detection (Using Imported Classes)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OOD DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize OOD detectors from ood_algorithms.py\n",
    "ood_detectors = {\n",
    "    'MaxSoftmax': MaxSoftmaxOOD(),\n",
    "    'ODIN (T=1000)': ODIN_OOD(temperature=1000),\n",
    "    'Energy': EnergyOOD(),\n",
    "}\n",
    "\n",
    "ood_results_dict = {}\n",
    "\n",
    "for source in CONFIG['datasets']:\n",
    "    id_key = f\"{source}_to_{source}\"\n",
    "    if id_key not in eval_results or eval_results[id_key]['logits'] is None:\n",
    "        continue\n",
    "    \n",
    "    id_logits = eval_results[id_key]['logits']\n",
    "    id_labels = eval_results[id_key]['labels']\n",
    "    \n",
    "    print(f\"\\n{source.upper()} as In-Distribution:\")\n",
    "    \n",
    "    for target in CONFIG['datasets']:\n",
    "        if target == source:\n",
    "            continue\n",
    "        \n",
    "        ood_key = f\"{source}_to_{target}\"\n",
    "        if ood_key not in eval_results or eval_results[ood_key]['logits'] is None:\n",
    "            continue\n",
    "        \n",
    "        ood_logits = eval_results[ood_key]['logits']\n",
    "        \n",
    "        # Binary labels: 1=ID, 0=OOD\n",
    "        y_true = np.concatenate([np.ones(len(id_logits)), np.zeros(len(ood_logits))])\n",
    "        all_logits = np.vstack([id_logits, ood_logits])\n",
    "        \n",
    "        print(f\"  vs {target} (OOD):\")\n",
    "        for name, detector in ood_detectors.items():\n",
    "            scores = -detector.compute_scores(all_logits)  # Negate: higher = more ID\n",
    "            auroc = roc_auc_score(y_true, scores)\n",
    "            \n",
    "            results.add('ood', method=name, source=source, target=target, auroc=auroc)\n",
    "            ood_results_dict[f\"{source}_vs_{target}_{name}\"] = auroc\n",
    "            \n",
    "            print(f\"    {name}: AUROC={auroc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ OOD detection complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Calibration Analysis (Using Imported Classes)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CALIBRATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "calibration_results = {}\n",
    "\n",
    "for source in roberta_models:\n",
    "    print(f\"\\n{source.upper()} Model:\")\n",
    "    \n",
    "    # Get validation data for fitting calibrators\n",
    "    val_key = f\"{source}_to_{source}\"\n",
    "    if val_key not in eval_results:\n",
    "        continue\n",
    "    \n",
    "    val_logits = eval_results[val_key]['logits']\n",
    "    val_labels = eval_results[val_key]['labels']\n",
    "    val_probs = eval_results[val_key]['probs']  # This is pos_probs (1D)\n",
    "    \n",
    "    if val_logits is None:\n",
    "        print(f\"  Skipping {source} - no logits available\")\n",
    "        continue\n",
    "    \n",
    "    # Fit temperature scaling using imported class\n",
    "    temp_scaler = TemperatureScaling()\n",
    "    temp_scaler.fit(val_logits, val_labels)\n",
    "    \n",
    "    # For isotonic, we need 2D probs - create from pos_probs\n",
    "    val_probs_2d = np.column_stack([1 - val_probs, val_probs])\n",
    "    iso_calib = IsotonicCalibration()\n",
    "    iso_calib.fit(val_probs_2d, val_labels)\n",
    "    \n",
    "    # Evaluate on all test sets\n",
    "    for target in CONFIG['datasets']:\n",
    "        test_key = f\"{source}_to_{target}\"\n",
    "        if test_key not in eval_results:\n",
    "            continue\n",
    "        \n",
    "        test_logits = eval_results[test_key]['logits']\n",
    "        test_labels = eval_results[test_key]['labels']\n",
    "        test_probs = eval_results[test_key]['probs']  # pos_probs (1D)\n",
    "        \n",
    "        if test_logits is None or test_probs is None:\n",
    "            continue\n",
    "        \n",
    "        # Uncalibrated ECE (using imported function)\n",
    "        ece_uncal, _ = expected_calibration_error(test_labels, test_probs)\n",
    "        \n",
    "        # Temperature Scaling\n",
    "        cal_probs_temp = temp_scaler.calibrate(test_logits)\n",
    "        ece_temp, _ = expected_calibration_error(test_labels, cal_probs_temp[:, 1])\n",
    "        \n",
    "        # Isotonic (need 2D input)\n",
    "        test_probs_2d = np.column_stack([1 - test_probs, test_probs])\n",
    "        cal_probs_iso = iso_calib.calibrate(test_probs_2d)\n",
    "        ece_iso, _ = expected_calibration_error(test_labels, cal_probs_iso[:, 1])\n",
    "        \n",
    "        print(f\"  {source}â†’{target}: ECE uncal={ece_uncal:.4f}, temp={ece_temp:.4f}, iso={ece_iso:.4f}\")\n",
    "        \n",
    "        results.add('calibration', source=source, target=target, method='Uncalibrated', ece=ece_uncal)\n",
    "        results.add('calibration', source=source, target=target, method='Temperature', ece=ece_temp)\n",
    "        results.add('calibration', source=source, target=target, method='Isotonic', ece=ece_iso)\n",
    "        \n",
    "        calibration_results[test_key] = {\n",
    "            'uncal': test_probs, \n",
    "            'temp': cal_probs_temp[:, 1], \n",
    "            'iso': cal_probs_iso[:, 1],\n",
    "            'labels': test_labels\n",
    "        }\n",
    "\n",
    "print(\"\\nâœ“ Calibration analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476e0e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Fairness Analysis\n",
    "\n",
    "Using `compute_group_fairness` from `scripts/fairness_metrics.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b108f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Fairness Analysis (Using Imported Function)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FAIRNESS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identity keywords for text-based detection\n",
    "IDENTITY_KEYWORDS = {\n",
    "    'male': ['male', 'man', 'men', 'boy', 'he ', 'him', 'his ', 'father', 'son', 'brother'],\n",
    "    'female': ['female', 'woman', 'women', 'girl', 'she ', 'her ', 'mother', 'daughter', 'sister'],\n",
    "    'lgbtq': ['gay', 'lesbian', 'bisexual', 'transgender', 'queer', 'lgbt'],\n",
    "    'christian': ['christian', 'christianity', 'church', 'catholic'],\n",
    "    'muslim': ['muslim', 'islam', 'islamic', 'mosque'],\n",
    "    'jewish': ['jewish', 'jew', 'judaism', 'synagogue'],\n",
    "    'black': ['black', 'african american'],\n",
    "    'white': ['white', 'caucasian'],\n",
    "}\n",
    "\n",
    "def detect_identity(text, keywords):\n",
    "    \"\"\"Check if text mentions identity group.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = str(text).lower()\n",
    "    return 1 if any(kw in text_lower for kw in keywords) else 0\n",
    "\n",
    "fairness_results = {}\n",
    "\n",
    "for source in roberta_models:\n",
    "    print(f\"\\n{source.upper()} Model:\")\n",
    "    \n",
    "    for target in CONFIG['datasets']:\n",
    "        test_key = f\"{source}_to_{target}\"\n",
    "        if test_key not in eval_results:\n",
    "            continue\n",
    "        \n",
    "        test_df = data[target]['test'].copy()\n",
    "        test_labels = eval_results[test_key]['labels']\n",
    "        test_probs = eval_results[test_key]['probs']\n",
    "        test_preds = (test_probs >= 0.5).astype(int)\n",
    "        \n",
    "        # Add predictions to dataframe\n",
    "        test_df = test_df.iloc[:len(test_labels)].copy()\n",
    "        test_df['label'] = test_labels\n",
    "        test_df['pred'] = test_preds\n",
    "        test_df['pos_prob'] = test_probs\n",
    "        \n",
    "        # Detect identity groups\n",
    "        group_cols = []\n",
    "        for group_name, keywords in IDENTITY_KEYWORDS.items():\n",
    "            col_name = f'g_{group_name}'\n",
    "            test_df[col_name] = test_df['text'].apply(lambda x: detect_identity(x, keywords))\n",
    "            if test_df[col_name].sum() > 50:  # Only include groups with enough samples\n",
    "                group_cols.append(col_name)\n",
    "        \n",
    "        if not group_cols:\n",
    "            print(f\"  {source}â†’{target}: No identity groups detected\")\n",
    "            continue\n",
    "        \n",
    "        # Use imported function\n",
    "        summary_df, per_group_df = compute_group_fairness(test_df, group_cols)\n",
    "        \n",
    "        fairness_results[test_key] = {'summary': summary_df, 'per_group': per_group_df}\n",
    "        \n",
    "        # Store results\n",
    "        for _, row in summary_df.iterrows():\n",
    "            group = row['group_col'].replace('g_', '')\n",
    "            results.add('fairness', source=source, target=target, group=group,\n",
    "                       dp_diff=row['dp_diff'], eop_diff=row['eop_diff'], eo_diff=row['eo_diff'])\n",
    "        \n",
    "        # Print summary\n",
    "        avg_dp = summary_df['dp_diff'].mean()\n",
    "        avg_eo = summary_df['eo_diff'].mean()\n",
    "        print(f\"  {source}â†’{target}: Avg DP Gap={avg_dp:.4f}, Avg EO Gap={avg_eo:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Fairness analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c7e4a",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Visualizations\n",
    "\n",
    "All plots for the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25028e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Plot Setup\n",
    "# =============================================================================\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'figure.figsize': (12, 8), 'font.size': 11, 'figure.dpi': 150})\n",
    "\n",
    "COLORS = {'jigsaw': '#1f77b4', 'civil': '#ff7f0e', 'hatexplain': '#2ca02c'}\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    for fmt in ['png', 'pdf']:\n",
    "        fig.savefig(FIGURES_DIR / f'{name}.{fmt}', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"  âœ“ Saved {name}\")\n",
    "\n",
    "print(\"âœ“ Plot settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Plot 1 - Classification Performance Heatmaps\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating Classification Heatmaps...\")\n",
    "\n",
    "class_df = results.get_df('classification')\n",
    "\n",
    "if len(class_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for ax, model in zip(axes, ['RoBERTa', 'TF-IDF-LOGREG']):\n",
    "        df = class_df[class_df['model'] == model]\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot = df.pivot_table(index='source', columns='target', values='f1', aggfunc='mean')\n",
    "        \n",
    "        im = ax.imshow(pivot.values, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "        ax.set_xticks(range(len(pivot.columns)))\n",
    "        ax.set_yticks(range(len(pivot.index)))\n",
    "        ax.set_xticklabels([c.capitalize() for c in pivot.columns])\n",
    "        ax.set_yticklabels([c.capitalize() for c in pivot.index])\n",
    "        ax.set_xlabel('Target')\n",
    "        ax.set_ylabel('Source')\n",
    "        ax.set_title(f'{model} F1 Score')\n",
    "        \n",
    "        # Annotate\n",
    "        for i in range(len(pivot.index)):\n",
    "            for j in range(len(pivot.columns)):\n",
    "                val = pivot.values[i, j]\n",
    "                if not np.isnan(val):\n",
    "                    ax.text(j, i, f'{val:.3f}', ha='center', va='center', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_fig(fig, 'classification_heatmaps')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"  No classification results to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Plot 2 - ROC & PR Curves\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating ROC & PR Curves...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx, source in enumerate(CONFIG['datasets']):\n",
    "    ax_roc = axes[0, idx]\n",
    "    ax_pr = axes[1, idx]\n",
    "    \n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    \n",
    "    for target in CONFIG['datasets']:\n",
    "        key = f\"{source}_to_{target}\"\n",
    "        if key not in eval_results or eval_results[key]['probs'] is None:\n",
    "            continue\n",
    "        \n",
    "        probs = eval_results[key]['probs']\n",
    "        labels = eval_results[key]['labels']\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, _ = roc_curve(labels, probs)\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "        style = '-' if source == target else '--'\n",
    "        ax_roc.plot(fpr, tpr, style, label=f'{target} (AUC={auc:.3f})', color=COLORS[target], linewidth=2)\n",
    "        \n",
    "        # PR\n",
    "        prec, rec, _ = precision_recall_curve(labels, probs)\n",
    "        ap = average_precision_score(labels, probs)\n",
    "        ax_pr.plot(rec, prec, style, label=f'{target} (AP={ap:.3f})', color=COLORS[target], linewidth=2)\n",
    "    \n",
    "    ax_roc.set_xlabel('FPR')\n",
    "    ax_roc.set_ylabel('TPR')\n",
    "    ax_roc.set_title(f'ROC: Trained on {source.capitalize()}')\n",
    "    ax_roc.legend(loc='lower right', fontsize=9)\n",
    "    \n",
    "    ax_pr.set_xlabel('Recall')\n",
    "    ax_pr.set_ylabel('Precision')\n",
    "    ax_pr.set_title(f'PR: Trained on {source.capitalize()}')\n",
    "    ax_pr.legend(loc='lower left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(fig, 'roc_pr_curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9dc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Plot 3 - Calibration Reliability Diagrams\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "print(\"Generating Calibration Plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, source in enumerate(CONFIG['datasets']):\n",
    "    ax = axes[idx]\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect', alpha=0.5)\n",
    "    \n",
    "    for target in CONFIG['datasets']:\n",
    "        key = f\"{source}_to_{target}\"\n",
    "        if key not in calibration_results:\n",
    "            continue\n",
    "        \n",
    "        probs = calibration_results[key]['uncal']\n",
    "        labels = calibration_results[key]['labels']\n",
    "        \n",
    "        if probs is None or len(probs) == 0:\n",
    "            continue\n",
    "        \n",
    "        prob_true, prob_pred = calibration_curve(labels, probs, n_bins=10, strategy='uniform')\n",
    "        style = '-o' if source == target else '--s'\n",
    "        ax.plot(prob_pred, prob_true, style, label=f'{target}', color=COLORS[target], markersize=5)\n",
    "    \n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.set_title(f'Calibration: {source.capitalize()} Model')\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(fig, 'calibration_reliability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: Plot 4 - OOD Detection & Fairness\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating OOD Detection & Fairness Plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# OOD Detection\n",
    "ood_df = results.get_df('ood')\n",
    "if len(ood_df) > 0:\n",
    "    ax = axes[0]\n",
    "    pivot = ood_df.pivot_table(index=['source', 'target'], columns='method', values='auroc')\n",
    "    pivot.plot(kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_xlabel('Source vs Target')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    ax.set_title('OOD Detection Performance')\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.set_xticklabels([f\"{s}â†’{t}\" for s, t in pivot.index], rotation=45, ha='right')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No OOD results', ha='center', va='center')\n",
    "    axes[0].set_title('OOD Detection')\n",
    "\n",
    "# Fairness\n",
    "fair_df = results.get_df('fairness')\n",
    "if len(fair_df) > 0:\n",
    "    ax = axes[1]\n",
    "    # Aggregate by source-target pair\n",
    "    agg = fair_df.groupby(['source', 'target'])[['dp_diff', 'eo_diff']].mean().reset_index()\n",
    "    agg['pair'] = agg['source'] + 'â†’' + agg['target']\n",
    "    \n",
    "    x = np.arange(len(agg))\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, agg['dp_diff'], width, label='DP Gap', color='steelblue')\n",
    "    ax.bar(x + width/2, agg['eo_diff'], width, label='EO Gap', color='coral')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(agg['pair'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Gap Value')\n",
    "    ax.set_title('Fairness Gaps (Lower is Better)')\n",
    "    ax.axhline(y=0.1, color='green', linestyle='--', alpha=0.5)\n",
    "    ax.legend()\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No fairness results', ha='center', va='center')\n",
    "    axes[1].set_title('Fairness Gaps')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(fig, 'ood_fairness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: Plot 5 - Training Curves\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating Training Curves...\")\n",
    "\n",
    "if training_histories:\n",
    "    n = len(training_histories)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(5*n, 4))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (source, hist) in zip(axes, training_histories.items()):\n",
    "        epochs = range(1, len(hist['train_loss']) + 1)\n",
    "        \n",
    "        ax.plot(epochs, hist['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "        ax.plot(epochs, hist['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(epochs, hist['val_f1'], 'g--', label='Val F1', linewidth=2)\n",
    "        ax2.set_ylabel('F1 Score', color='green')\n",
    "        ax2.tick_params(axis='y', labelcolor='green')\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title(f'Training: {source.capitalize()}')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_fig(fig, 'training_curves')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"  No training histories to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302bdef",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Results Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: Results Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification\n",
    "print(\"\\nðŸ“Š CLASSIFICATION PERFORMANCE\")\n",
    "class_df = results.get_df('classification')\n",
    "if len(class_df) > 0:\n",
    "    roberta = class_df[class_df['model'] == 'RoBERTa']\n",
    "    if len(roberta) > 0:\n",
    "        print(\"\\nRoBERTa Cross-Domain F1:\")\n",
    "        display(roberta.pivot_table(index='source', columns='target', values='f1').round(4))\n",
    "        \n",
    "        cross = roberta[roberta['source'] != roberta['target']]\n",
    "        if len(cross) > 0:\n",
    "            best = cross.loc[cross['f1'].idxmax()]\n",
    "            print(f\"\\nâœ“ Best Cross-Domain: {best['source']}â†’{best['target']}: F1={best['f1']:.4f}\")\n",
    "\n",
    "# OOD Detection\n",
    "print(\"\\n\\nðŸ” OOD DETECTION\")\n",
    "ood_df = results.get_df('ood')\n",
    "if len(ood_df) > 0:\n",
    "    display(ood_df.pivot_table(index=['source', 'target'], columns='method', values='auroc').round(4))\n",
    "\n",
    "# Calibration\n",
    "print(\"\\n\\nðŸ“ CALIBRATION (ECE)\")\n",
    "cal_df = results.get_df('calibration')\n",
    "if len(cal_df) > 0:\n",
    "    display(cal_df.pivot_table(index=['source', 'target'], columns='method', values='ece').round(4))\n",
    "\n",
    "# Fairness\n",
    "print(\"\\n\\nâš–ï¸ FAIRNESS\")\n",
    "fair_df = results.get_df('fairness')\n",
    "if len(fair_df) > 0:\n",
    "    agg = fair_df.groupby(['source', 'target'])[['dp_diff', 'eo_diff']].mean()\n",
    "    display(agg.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 18: Export Results\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save JSON\n",
    "results.save(RESULTS_DIR / 'all_results.json')\n",
    "print(f\"âœ“ JSON: {RESULTS_DIR / 'all_results.json'}\")\n",
    "\n",
    "# Save CSVs\n",
    "for name in ['classification', 'ood', 'calibration', 'fairness']:\n",
    "    df = results.get_df(name)\n",
    "    if len(df) > 0:\n",
    "        df.to_csv(RESULTS_DIR / f'{name}_results.csv', index=False)\n",
    "        print(f\"âœ“ CSV: {name}_results.csv ({len(df)} rows)\")\n",
    "\n",
    "# Save training histories\n",
    "if training_histories:\n",
    "    with open(RESULTS_DIR / 'training_histories.json', 'w') as f:\n",
    "        json.dump({k: {kk: [float(x) for x in vv] for kk, vv in v.items()} \n",
    "                   for k, v in training_histories.items()}, f, indent=2)\n",
    "    print(f\"âœ“ Training histories saved\")\n",
    "\n",
    "# List figures\n",
    "figs = list(FIGURES_DIR.glob('*.png'))\n",
    "print(f\"\\nâœ“ {len(figs)} figures saved to {FIGURES_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
