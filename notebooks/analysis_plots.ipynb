{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Cell - Add this at the top of each notebook\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Set base directories based on environment\n",
    "if IS_KAGGLE:\n",
    "    INPUT_ROOT = \"/kaggle/input\"\n",
    "    WORK_DIR = \"/kaggle/working\"\n",
    "elif IS_COLAB:\n",
    "    INPUT_ROOT = \"/content/input\"\n",
    "    WORK_DIR = \"/content/working\"\n",
    "else:\n",
    "    # Local environment\n",
    "    INPUT_ROOT = Path.cwd() / \"input\"\n",
    "    WORK_DIR = Path.cwd() / \"working\"\n",
    "\n",
    "# Create standard directories\n",
    "OUT_DIR = os.path.join(WORK_DIR, \"data\")\n",
    "EXPERIMENTS_DIR = os.path.join(WORK_DIR, \"experiments\")\n",
    "SCRIPTS_DIR = os.path.join(WORK_DIR, \"scripts\")\n",
    "\n",
    "# Create all directories\n",
    "for directory in [OUT_DIR, EXPERIMENTS_DIR, SCRIPTS_DIR]:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "print(f\"Input directory: {INPUT_ROOT}\")\n",
    "print(f\"Working directory: {WORK_DIR}\")\n",
    "print(f\"Data directory: {OUT_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d78b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and Visualization Notebook\n",
    "# Run this after training models and computing fairness metrics\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Directories\n",
    "EXPERIMENTS_DIR = \"/kaggle/working/experiments\"\n",
    "PLOTS_DIR = os.path.join(EXPERIMENTS_DIR, \"plots\")\n",
    "\n",
    "# FIXED: Ensure directories exist\n",
    "Path(EXPERIMENTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PLOTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Analysis and Plotting Notebook\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")\n",
    "print(f\"Plots will be saved to: {PLOTS_DIR}\")\n",
    "\n",
    "# FIXED: Check if experiments directory has any data\n",
    "exp_files = list(Path(EXPERIMENTS_DIR).glob(\"*.csv\"))\n",
    "if not exp_files:\n",
    "    print(\"\\n⚠️  WARNING: No experiment files found!\")\n",
    "    print(\"Please run training scripts first:\")\n",
    "    print(\"  - run_roberta.py\")\n",
    "    print(\"  - run_tfidf_baselines.py\")\n",
    "else:\n",
    "    print(f\"\\n✓ Found {len(exp_files)} experiment files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary metrics from experiments\n",
    "def load_summary_metrics(source_dataset, model_type=\"roberta\"):\n",
    "    \"\"\"Load summary CSV for a given source and model type.\"\"\"\n",
    "    if model_type == \"roberta\":\n",
    "        path = Path(EXPERIMENTS_DIR) / f\"summary_{source_dataset}.csv\"\n",
    "    else:\n",
    "        path = Path(EXPERIMENTS_DIR) / f\"summary_tfidf_{source_dataset}_{model_type}.csv\"\n",
    "    \n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {path.name}: {len(df)} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"[WARN] File not found: {path}\")\n",
    "        return None\n",
    "\n",
    "# Example: Load Jigsaw summaries\n",
    "jigsaw_roberta = load_summary_metrics(\"jigsaw\", \"roberta\")\n",
    "jigsaw_tfidf_lr = load_summary_metrics(\"jigsaw\", \"logreg\")\n",
    "\n",
    "if jigsaw_roberta is not None:\n",
    "    display(jigsaw_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RELIABILITY DIAGRAMS\n",
    "\n",
    "def plot_reliability_diagram(csv_path, title=\"Reliability Diagram\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot reliability diagram from binned statistics CSV.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to reliability CSV with columns: bin, left, right, count, acc, conf, gap\n",
    "        title: Plot title\n",
    "        save_path: Where to save the plot (if None, just display)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Filter out empty bins\n",
    "    df = df[df['count'] > 0].copy()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"[WARN] No valid bins in {csv_path}\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Plot points\n",
    "    ax.scatter(df['conf'], df['acc'], s=df['count']/df['count'].max()*500, \n",
    "               alpha=0.6, c='steelblue', edgecolors='black', linewidths=1)\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Perfect Calibration')\n",
    "    \n",
    "    # Labels and formatting\n",
    "    ax.set_xlabel('Confidence (Mean Predicted Probability)', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy (Fraction of Positives)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot reliability diagram for Jigsaw validation\n",
    "reliability_csv = Path(EXPERIMENTS_DIR) / \"jigsaw_val_reliability.csv\"\n",
    "if reliability_csv.exists():\n",
    "    plot_reliability_diagram(\n",
    "        reliability_csv,\n",
    "        title=\"Reliability Diagram: Jigsaw Validation Set\",\n",
    "        save_path=os.path.join(PLOTS_DIR, \"reliability_jigsaw_val.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ROC CURVES\n",
    "\n",
    "def plot_roc_curve(pred_csv_path, title=\"ROC Curve\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curve from predictions CSV.\n",
    "    \n",
    "    Args:\n",
    "        pred_csv_path: Path to predictions CSV with columns: label, score (or pos_prob)\n",
    "        title: Plot title\n",
    "        save_path: Where to save\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(pred_csv_path)\n",
    "    \n",
    "    # Determine score column\n",
    "    score_col = \"pos_prob\" if \"pos_prob\" in df.columns else \"score\"\n",
    "    \n",
    "    if score_col not in df.columns or \"label\" not in df.columns:\n",
    "        print(f\"[WARN] Missing required columns in {pred_csv_path}\")\n",
    "        return\n",
    "    \n",
    "    y_true = df[\"label\"].values\n",
    "    y_score = df[score_col].values\n",
    "    \n",
    "    # Compute ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    \n",
    "    from sklearn.metrics import auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc=\"lower right\", fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot ROC for in-domain test\n",
    "test_preds = Path(EXPERIMENTS_DIR) / \"preds_jigsaw_test.csv\"\n",
    "if test_preds.exists():\n",
    "    plot_roc_curve(\n",
    "        test_preds,\n",
    "        title=\"ROC Curve: Jigsaw Test Set (RoBERTa)\",\n",
    "        save_path=os.path.join(PLOTS_DIR, \"roc_jigsaw_test.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1babd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PRECISION-RECALL CURVES\n",
    "\n",
    "def plot_pr_curve(pred_csv_path, title=\"Precision-Recall Curve\", save_path=None):\n",
    "    \"\"\"Plot Precision-Recall curve from predictions CSV.\"\"\"\n",
    "    df = pd.read_csv(pred_csv_path)\n",
    "    \n",
    "    score_col = \"pos_prob\" if \"pos_prob\" in df.columns else \"score\"\n",
    "    \n",
    "    if score_col not in df.columns or \"label\" not in df.columns:\n",
    "        print(f\"[WARN] Missing required columns in {pred_csv_path}\")\n",
    "        return\n",
    "    \n",
    "    y_true = df[\"label\"].values\n",
    "    y_score = df[score_col].values\n",
    "    \n",
    "    # Compute PR curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "    \n",
    "    from sklearn.metrics import average_precision_score\n",
    "    avg_precision = average_precision_score(y_true, y_score)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(recall, precision, color='purple', lw=2, \n",
    "            label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Recall', fontsize=12)\n",
    "    ax.set_ylabel('Precision', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc=\"lower left\", fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot PR curve\n",
    "if test_preds.exists():\n",
    "    plot_pr_curve(\n",
    "        test_preds,\n",
    "        title=\"Precision-Recall Curve: Jigsaw Test Set (RoBERTa)\",\n",
    "        save_path=os.path.join(PLOTS_DIR, \"pr_jigsaw_test.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e346d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CONFUSION MATRICES\n",
    "\n",
    "def plot_confusion_matrix(pred_csv_path, title=\"Confusion Matrix\", save_path=None):\n",
    "    \"\"\"Plot confusion matrix heatmap from predictions CSV.\"\"\"\n",
    "    df = pd.read_csv(pred_csv_path)\n",
    "    \n",
    "    if \"label\" not in df.columns or \"pred\" not in df.columns:\n",
    "        print(f\"[WARN] Missing 'label' or 'pred' columns in {pred_csv_path}\")\n",
    "        return\n",
    "    \n",
    "    y_true = df[\"label\"].values\n",
    "    y_pred = df[\"pred\"].values\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Toxic', 'Toxic'],\n",
    "                yticklabels=['Non-Toxic', 'Toxic'],\n",
    "                cbar_kws={'label': 'Count'},\n",
    "                ax=ax)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example: Confusion matrix for test set\n",
    "if test_preds.exists():\n",
    "    plot_confusion_matrix(\n",
    "        test_preds,\n",
    "        title=\"Confusion Matrix: Jigsaw Test Set (RoBERTa)\",\n",
    "        save_path=os.path.join(PLOTS_DIR, \"cm_jigsaw_test.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CROSS-DOMAIN COMPARISON\n",
    "\n",
    "def plot_cross_domain_comparison(summaries_dict, metric=\"f1\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot comparison of in-domain vs cross-domain performance.\n",
    "    \n",
    "    Args:\n",
    "        summaries_dict: Dict mapping model_name -> summary DataFrame\n",
    "        metric: Metric to plot (f1, accuracy, auroc)\n",
    "        save_path: Where to save\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x_labels = []\n",
    "    width = 0.15\n",
    "    x_pos = np.arange(len(summaries_dict))\n",
    "    \n",
    "    for idx, (model_name, summary_df) in enumerate(summaries_dict.items()):\n",
    "            if summary_df is None:\n",
    "                continue\n",
    "            \n",
    "            values = []\n",
    "            labels = []\n",
    "            \n",
    "            for _, row in summary_df.iterrows():\n",
    "                split = row['split']\n",
    "                val = row.get(metric, np.nan)\n",
    "                \n",
    "                if not np.isnan(val):\n",
    "                    values.append(val)\n",
    "                    labels.append(split)\n",
    "            \n",
    "            if not x_labels:\n",
    "                x_labels = labels\n",
    "            \n",
    "            x = np.arange(len(values)) + idx * width\n",
    "            ax.bar(x, values, width, label=model_name, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Dataset Split', fontsize=12)\n",
    "        ax.set_ylabel(metric.upper(), fontsize=12)\n",
    "        ax.set_title(f'Cross-Domain Performance Comparison: {metric.upper()}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(np.arange(len(x_labels)) + width * (len(summaries_dict) - 1) / 2)\n",
    "        ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    # Example: Compare RoBERTa and TF-IDF\n",
    "    if jigsaw_roberta is not None and jigsaw_tfidf_lr is not None:\n",
    "        plot_cross_domain_comparison(\n",
    "            {\n",
    "                \"RoBERTa\": jigsaw_roberta,\n",
    "                \"TF-IDF LR\": jigsaw_tfidf_lr\n",
    "            },\n",
    "            metric=\"f1\",\n",
    "            save_path=os.path.join(PLOTS_DIR, \"comparison_f1.png\")\n",
    "        )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. FAIRNESS METRICS VISUALIZATION\n",
    "\n",
    "def plot_fairness_summary(fairness_csv_path, title=\"Fairness Metrics\", \n",
    "                          top_k=10, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot fairness metrics from summary CSV.\n",
    "    \n",
    "    Args:\n",
    "        fairness_csv_path: Path to fairness summary CSV\n",
    "        title: Plot title\n",
    "        top_k: Number of top groups to show\n",
    "        save_path: Where to save\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fairness_csv_path)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"[WARN] Empty fairness summary: {fairness_csv_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get top k groups by DP difference\n",
    "    df_sorted = df.nlargest(top_k, 'dp_diff')\n",
    "    \n",
    "    # Clean group names (remove 'g_' prefix)\n",
    "    df_sorted['group_name'] = df_sorted['group_col'].str.replace('g_', '')\n",
    "    \n",
    "    # Create subplots for each fairness metric\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    metrics = ['dp_diff', 'eop_diff', 'eo_diff']\n",
    "    titles = ['Demographic Parity Gap', 'Equal Opportunity Gap', 'Equalized Odds Gap']\n",
    "    colors = ['steelblue', 'orange', 'green']\n",
    "    \n",
    "    for ax, metric, plot_title, color in zip(axes, metrics, titles, colors):\n",
    "        df_metric = df.nlargest(top_k, metric)\n",
    "        df_metric['group_name'] = df_metric['group_col'].str.replace('g_', '')\n",
    "        \n",
    "        ax.barh(df_metric['group_name'], df_metric[metric], color=color, alpha=0.7)\n",
    "        ax.set_xlabel('Difference', fontsize=11)\n",
    "        ax.set_title(plot_title, fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        ax.invert_yaxis()\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot fairness metrics\n",
    "fairness_summary = Path(EXPERIMENTS_DIR) / \"fairness_jigsaw_test_summary.csv\"\n",
    "if fairness_summary.exists():\n",
    "    plot_fairness_summary(\n",
    "        fairness_summary,\n",
    "        title=\"Fairness Metrics: Jigsaw Test Set\",\n",
    "        top_k=10,\n",
    "        save_path=os.path.join(PLOTS_DIR, \"fairness_jigsaw_test.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. PER-GROUP FAIRNESS COMPARISON\n",
    "\n",
    "def plot_per_group_metrics(fairness_per_group_csv, groups_to_plot=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot per-group metrics (TPR, FPR, positive rate).\n",
    "    \n",
    "    Args:\n",
    "        fairness_per_group_csv: Path to per-group CSV\n",
    "        groups_to_plot: List of group names to plot (or None for auto-select)\n",
    "        save_path: Where to save\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fairness_per_group_csv)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"[WARN] Empty per-group data\")\n",
    "        return\n",
    "    \n",
    "    # Auto-select interesting groups if not specified\n",
    "    if groups_to_plot is None:\n",
    "        # Select groups with sufficient support and variation\n",
    "        group_stats = df.groupby('group_col').agg({\n",
    "            'support': 'sum',\n",
    "            'tpr': lambda x: x.std(skipna=True)\n",
    "        })\n",
    "        group_stats = group_stats[group_stats['support'] > 100]\n",
    "        groups_to_plot = group_stats.nlargest(6, 'tpr').index.tolist()\n",
    "    \n",
    "    # Filter data\n",
    "    df_plot = df[df['group_col'].isin(groups_to_plot)].copy()\n",
    "    df_plot['group_name'] = df_plot['group_col'].str.replace('g_', '')\n",
    "    df_plot['group_val_str'] = df_plot['group_val'].map({0: 'Not in group', 1: 'In group'})\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    metrics = ['pos_rate', 'tpr', 'fpr']\n",
    "    titles = ['Positive Prediction Rate', 'True Positive Rate', 'False Positive Rate']\n",
    "    \n",
    "    for ax, metric, plot_title in zip(axes, metrics, titles):\n",
    "        pivot = df_plot.pivot_table(\n",
    "            values=metric,\n",
    "            index='group_name',\n",
    "            columns='group_val_str',\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        pivot.plot(kind='bar', ax=ax, rot=45, alpha=0.8, width=0.8)\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_title(plot_title, fontsize=12, fontweight='bold')\n",
    "        ax.legend(title='', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot per-group metrics\n",
    "per_group_csv = Path(EXPERIMENTS_DIR) / \"fairness_jigsaw_test_per_group.csv\"\n",
    "if per_group_csv.exists():\n",
    "    plot_per_group_metrics(\n",
    "        per_group_csv,\n",
    "        groups_to_plot=['g_male', 'g_female', 'g_black', 'g_white', 'g_lgbtq', 'g_muslim'],\n",
    "        save_path=os.path.join(PLOTS_DIR, \"per_group_jigsaw_test.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db987218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. CALIBRATION COMPARISON (Before/After)\n",
    "\n",
    "def plot_calibration_comparison(bins_before_csv, bins_after_csv, save_path=None):\n",
    "    \"\"\"\n",
    "    Compare reliability before and after calibration.\n",
    "    \n",
    "    Args:\n",
    "        bins_before_csv: Path to reliability bins CSV before calibration\n",
    "        bins_after_csv: Path to reliability bins CSV after calibration\n",
    "        save_path: Where to save\n",
    "    \"\"\"\n",
    "    df_before = pd.read_csv(bins_before_csv)\n",
    "    df_after = pd.read_csv(bins_after_csv)\n",
    "    \n",
    "    # Filter non-empty bins\n",
    "    df_before = df_before[df_before['count'] > 0]\n",
    "    df_after = df_after[df_after['count'] > 0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    for ax, df, title in zip(axes, [df_before, df_after], \n",
    "                              ['Before Calibration', 'After Calibration']):\n",
    "        ax.scatter(df['conf'], df['acc'], \n",
    "                   s=df['count']/df['count'].max()*500,\n",
    "                   alpha=0.6, c='steelblue', edgecolors='black', linewidths=1)\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Perfect Calibration')\n",
    "        ax.set_xlabel('Confidence', fontsize=12)\n",
    "        ax.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        # Add ECE annotation\n",
    "        ece = df['gap'].mean()\n",
    "        ax.text(0.05, 0.95, f'ECE = {ece:.4f}', \n",
    "                transform=ax.transAxes, fontsize=11,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Note: You would need to save reliability bins before and after calibration\n",
    "# This is just an example structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b248fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. AGGREGATE METRICS TABLE\n",
    "\n",
    "def create_metrics_table(summary_dfs_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a formatted table of metrics across models/datasets.\n",
    "    \n",
    "    Args:\n",
    "        summary_dfs_dict: Dict mapping model_name -> summary DataFrame\n",
    "        save_path: Path to save CSV (optional)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name, df in summary_dfs_dict.items():\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            rows.append({\n",
    "                'Model': model_name,\n",
    "                'Split': row['split'],\n",
    "                'Accuracy': f\"{row.get('accuracy', np.nan):.4f}\",\n",
    "                'F1': f\"{row.get('f1', np.nan):.4f}\",\n",
    "                'AUROC': f\"{row.get('auroc', np.nan):.4f}\",\n",
    "                'PR-AUC': f\"{row.get('pr_auc', np.nan):.4f}\",\n",
    "                'ECE': f\"{row.get('ece', np.nan):.4f}\",\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(rows)\n",
    "    \n",
    "    if save_path:\n",
    "        results_df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved metrics table to: {save_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example: Create comprehensive metrics table\n",
    "metrics_table = create_metrics_table(\n",
    "    {\n",
    "        \"RoBERTa\": jigsaw_roberta,\n",
    "        \"TF-IDF LR\": jigsaw_tfidf_lr,\n",
    "    },\n",
    "    save_path=os.path.join(EXPERIMENTS_DIR, \"metrics_table.csv\")\n",
    ")\n",
    "\n",
    "display(metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. SUMMARY REPORT\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# List all generated plots\n",
    "plot_files = list(Path(PLOTS_DIR).glob(\"*.png\"))\n",
    "print(f\"Generated {len(plot_files)} plots:\")\n",
    "for pf in sorted(plot_files):\n",
    "    print(f\"  - {pf.name}\")\n",
    "\n",
    "print()\n",
    "print(\"All analysis complete! Plots saved to:\", PLOTS_DIR)\n",
    "print()\n",
    "print(\"Key files for report:\")\n",
    "print(\"  1. Reliability diagrams: reliability_*.png\")\n",
    "print(\"  2. ROC curves: roc_*.png\")\n",
    "print(\"  3. PR curves: pr_*.png\")\n",
    "print(\"  4. Confusion matrices: cm_*.png\")\n",
    "print(\"  5. Fairness analysis: fairness_*.png\")\n",
    "print(\"  6. Cross-domain comparison: comparison_*.png\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  - Copy plots into your project report/presentation\")\n",
    "print(\"  - Use metrics_table.csv for quantitative results\")\n",
    "print(\"  - Analyze fairness metrics to identify bias patterns\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
