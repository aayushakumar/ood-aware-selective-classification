{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f178439",
   "metadata": {},
   "source": [
    "# OOD Evaluation of Toxic Comment Classifiers\n",
    "## Final Project Experiments & Results\n",
    "\n",
    "This notebook reproduces all experiments, results, and plots for the project.\n",
    "It covers:\n",
    "1.  **Setup**: Environment and Data.\n",
    "2.  **Baselines**: TF-IDF + Logistic Regression / SVM.\n",
    "3.  **Models**: RoBERTa (In-domain and Cross-domain).\n",
    "4.  **Fairness**: Demographic Parity and Equal Opportunity analysis.\n",
    "5.  **Analysis**: Final plots and tables.\n",
    "\n",
    "**Datasets**: Civil Comments, HateXplain, Jigsaw.\n",
    "**Goal**: Evaluate OOD generalization and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Setup & Installation\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running in Local Environment\")\n",
    "\n",
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    print(\"Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    # Install specific versions if needed, e.g. transformers\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"scikit-learn\", \"pandas\", \"matplotlib\", \"seaborn\", \"torch\"])\n",
    "\n",
    "# Mount Drive if needed (optional)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "REPO_DIR = Path(os.getcwd())\n",
    "if IN_COLAB:\n",
    "    # Assuming repo is cloned to /content/ood-eval-toxic-classifiers or similar\n",
    "    # If not, clone it:\n",
    "    if not (REPO_DIR / \"scripts\").exists():\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/aayushakumar/ood-eval-toxic-classifiers.git\n",
    "        os.chdir(\"ood-eval-toxic-classifiers\")\n",
    "        REPO_DIR = Path(os.getcwd())\n",
    "\n",
    "DATA_DIR = REPO_DIR / \"data\"\n",
    "EXPERIMENTS_DIR = REPO_DIR / \"experiments\"\n",
    "SCRIPTS_DIR = REPO_DIR / \"scripts\"\n",
    "\n",
    "EXPERIMENTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Working Directory: {REPO_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Experiments Directory: {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c05acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Data Verification\n",
    "import pandas as pd\n",
    "\n",
    "REQUIRED_FILES = [\n",
    "    \"civil_train.csv\", \"civil_val.csv\", \"civil_test.csv\",\n",
    "    \"hatexplain_train.csv\", \"hatexplain_val.csv\", \"hatexplain_test.csv\",\n",
    "    \"jigsaw_train.csv\", \"jigsaw_val.csv\", \"jigsaw_test.csv\"\n",
    "]\n",
    "\n",
    "FULL_FILES = [\n",
    "    \"jigsaw_test_full.csv\",  # Required for fairness analysis\n",
    "    \"jigsaw_train_full.csv\",\n",
    "    \"jigsaw_val_full.csv\",\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for fname in REQUIRED_FILES:\n",
    "    if not (DATA_DIR / fname).exists():\n",
    "        missing_files.append(fname)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"❌ WARNING: Missing data files: {missing_files}\")\n",
    "    print(\"Please ensure data is uploaded to the 'data/' directory.\")\n",
    "else:\n",
    "    print(\"✓ All required data files found.\")\n",
    "\n",
    "# Check full files for fairness\n",
    "missing_full = []\n",
    "for fname in FULL_FILES:\n",
    "    if not (DATA_DIR / fname).exists():\n",
    "        missing_full.append(fname)\n",
    "\n",
    "if missing_full:\n",
    "    print(f\"\\n⚠️  Missing full data files (needed for fairness): {missing_full}\")\n",
    "else:\n",
    "    print(\"✓ Full data files for fairness analysis found.\")\n",
    "\n",
    "# Dataset statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset in [\"civil\", \"hatexplain\", \"jigsaw\"]:\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        fpath = DATA_DIR / f\"{dataset}_{split}.csv\"\n",
    "        if fpath.exists():\n",
    "            df = pd.read_csv(fpath)\n",
    "            n_toxic = (df[\"label\"] == 1).sum()\n",
    "            n_total = len(df)\n",
    "            print(f\"  {split:5s}: {n_total:6d} samples, {n_toxic:5d} toxic ({100*n_toxic/n_total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a650426",
   "metadata": {},
   "source": [
    "## 2.5 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize dataset characteristics: label distributions, text lengths, and class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title EDA: Dataset Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "datasets_info = []\n",
    "\n",
    "for idx, dataset in enumerate([\"civil\", \"hatexplain\", \"jigsaw\"]):\n",
    "    # Load train data\n",
    "    train_path = DATA_DIR / f\"{dataset}_train.csv\"\n",
    "    if not train_path.exists():\n",
    "        continue\n",
    "    df = pd.read_csv(train_path)\n",
    "    \n",
    "    # Text lengths\n",
    "    df[\"text_len\"] = df[\"text\"].astype(str).apply(len)\n",
    "    df[\"word_count\"] = df[\"text\"].astype(str).apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # Store stats\n",
    "    datasets_info.append({\n",
    "        \"Dataset\": dataset,\n",
    "        \"Total\": len(df),\n",
    "        \"Toxic\": (df[\"label\"] == 1).sum(),\n",
    "        \"Non-Toxic\": (df[\"label\"] == 0).sum(),\n",
    "        \"Toxic %\": 100 * (df[\"label\"] == 1).mean(),\n",
    "        \"Avg Words\": df[\"word_count\"].mean(),\n",
    "        \"Avg Chars\": df[\"text_len\"].mean()\n",
    "    })\n",
    "    \n",
    "    # Plot 1: Label distribution\n",
    "    ax1 = axes[0, idx]\n",
    "    labels, counts = np.unique(df[\"label\"], return_counts=True)\n",
    "    colors = [\"#2ecc71\", \"#e74c3c\"]\n",
    "    ax1.bar([\"Non-Toxic\", \"Toxic\"], counts, color=colors)\n",
    "    ax1.set_title(f\"{dataset.upper()}: Label Distribution\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "    for i, (l, c) in enumerate(zip(labels, counts)):\n",
    "        ax1.text(i, c + 100, f\"{c:,}\", ha=\"center\", fontsize=10)\n",
    "    \n",
    "    # Plot 2: Text length distribution\n",
    "    ax2 = axes[1, idx]\n",
    "    ax2.hist(df[\"word_count\"], bins=50, alpha=0.7, color=\"#3498db\", edgecolor=\"black\")\n",
    "    ax2.axvline(df[\"word_count\"].mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {df['word_count'].mean():.1f}\")\n",
    "    ax2.set_title(f\"{dataset.upper()}: Word Count Distribution\")\n",
    "    ax2.set_xlabel(\"Word Count\")\n",
    "    ax2.set_ylabel(\"Frequency\")\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(0, 200)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EXPERIMENTS_DIR / \"eda_plots.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "info_df = pd.DataFrame(datasets_info)\n",
    "print(info_df.to_string(index=False))\n",
    "print(f\"\\nSaved: {EXPERIMENTS_DIR / 'eda_plots.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7ac50",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Baselines\n",
    "\n",
    "We train Logistic Regression and Linear SVM models on each source dataset and evaluate on all target datasets.\n",
    "Metrics: Accuracy, F1, ROC-AUC, PR-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ccaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run TF-IDF Experiments\n",
    "RUN_TFIDF = True # @param {type:\"boolean\"}\n",
    "\n",
    "datasets = [\"civil\", \"hatexplain\", \"jigsaw\"]\n",
    "\n",
    "if RUN_TFIDF:\n",
    "    print(\"Running TF-IDF Baselines...\")\n",
    "    for source in datasets:\n",
    "        # Target all other datasets + self\n",
    "        targets = [d for d in datasets]\n",
    "        \n",
    "        print(f\"\\n--- Training on {source} ---\")\n",
    "        cmd = [\n",
    "            sys.executable, \"scripts/run_tfidf_baselines.py\",\n",
    "            \"--source_dataset\", source,\n",
    "            \"--target_datasets\"\n",
    "        ] + targets + [\n",
    "            \"--model\", \"both\",  # Run both LogReg and SVM\n",
    "            \"--save_preds\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"Executing: {' '.join(cmd)}\")\n",
    "        subprocess.check_call(cmd)\n",
    "        \n",
    "    print(\"\\nTF-IDF Experiments Completed.\")\n",
    "else:\n",
    "    print(\"Skipping TF-IDF experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef3a65",
   "metadata": {},
   "source": [
    "## 4. RoBERTa Experiments\n",
    "\n",
    "We train RoBERTa-base models.\n",
    "Options:\n",
    "- **Standard**: Fine-tuning on source.\n",
    "- **CORAL**: Domain adaptation (requires unlabeled target).\n",
    "- **LoRA**: Parameter-efficient fine-tuning.\n",
    "\n",
    "We also enable **Calibration** (Temperature Scaling) and save predictions for fairness analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run RoBERTa Experiments\n",
    "RUN_ROBERTA = True # @param {type:\"boolean\"}\n",
    "USE_CORAL = False # @param {type:\"boolean\"}\n",
    "USE_LORA = False # @param {type:\"boolean\"}\n",
    "FAST_MODE = True # @param {type:\"boolean\"}\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 1 if FAST_MODE else 3\n",
    "SEEDS = [42] # Add more seeds for full paper results, e.g. [42, 123, 456]\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "if RUN_ROBERTA:\n",
    "    print(\"Running RoBERTa Experiments...\")\n",
    "    \n",
    "    for source in datasets:\n",
    "        targets = [d for d in datasets] # All datasets\n",
    "        \n",
    "        # Base arguments\n",
    "        cmd = [\n",
    "            sys.executable, \"scripts/run_roberta.py\",\n",
    "            \"--source_dataset\", source,\n",
    "            \"--model_name\", \"roberta-base\",\n",
    "            \"--epochs\", str(EPOCHS),\n",
    "            \"--batch_size\", str(BATCH_SIZE),\n",
    "            \"--max_len\", str(MAX_LEN),\n",
    "            \"--seeds\"\n",
    "        ] + [str(s) for s in SEEDS] + [\n",
    "            \"--target_datasets\"\n",
    "        ] + targets + [\n",
    "            \"--calibration\", \"temperature\", # Enable calibration\n",
    "            \"--save_preds\",\n",
    "            \"--amp\", # Use Mixed Precision\n",
    "            \"--tune_threshold\"\n",
    "        ]\n",
    "        \n",
    "        # LoRA\n",
    "        if USE_LORA:\n",
    "            cmd += [\"--peft\", \"lora\"]\n",
    "            \n",
    "        # CORAL (Example: if source is civil, adapt to jigsaw)\n",
    "        # Note: CORAL requires a specific target. For simplicity, we run standard here.\n",
    "        # To run CORAL, you would need a separate loop or logic.\n",
    "        \n",
    "        print(f\"\\n--- Training RoBERTa on {source} ---\")\n",
    "        print(f\"Executing: {' '.join(cmd)}\")\n",
    "        \n",
    "        # Run\n",
    "        subprocess.check_call(cmd)\n",
    "        \n",
    "    print(\"\\nRoBERTa Experiments Completed.\")\n",
    "else:\n",
    "    print(\"Skipping RoBERTa experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382476e",
   "metadata": {},
   "source": [
    "## 5. Fairness Analysis\n",
    "\n",
    "We compute fairness metrics (Demographic Parity, Equal Opportunity) for models on datasets with identity attributes (Jigsaw, Civil).\n",
    "We use the `scripts/fairness_metrics.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45962c77",
   "metadata": {},
   "source": [
    "## 4.5 Calibration Analysis\n",
    "\n",
    "Analyze model calibration using Expected Calibration Error (ECE) and reliability diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Calibration Analysis: ECE and Reliability Diagrams\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"Compute Expected Calibration Error.\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (y_prob > bin_boundaries[i]) & (y_prob <= bin_boundaries[i + 1])\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        if prop_in_bin > 0:\n",
    "            avg_confidence = y_prob[in_bin].mean()\n",
    "            avg_accuracy = y_true[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence - avg_accuracy) * prop_in_bin\n",
    "    return ece\n",
    "\n",
    "# Find prediction files\n",
    "pred_files = list(EXPERIMENTS_DIR.glob(\"preds_*_test.csv\"))\n",
    "\n",
    "if pred_files:\n",
    "    fig, axes = plt.subplots(1, len(pred_files), figsize=(5 * len(pred_files), 5))\n",
    "    if len(pred_files) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    calibration_results = []\n",
    "    \n",
    "    for ax, pred_file in zip(axes, pred_files):\n",
    "        try:\n",
    "            df = pd.read_csv(pred_file)\n",
    "            if \"pos_prob\" not in df.columns and \"score\" not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            prob_col = \"pos_prob\" if \"pos_prob\" in df.columns else \"score\"\n",
    "            y_true = df[\"label\"].values\n",
    "            y_prob = df[prob_col].values\n",
    "            \n",
    "            # Compute calibration curve\n",
    "            prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10, strategy=\"uniform\")\n",
    "            \n",
    "            # Compute ECE\n",
    "            ece = compute_ece(y_true, y_prob)\n",
    "            \n",
    "            # Plot\n",
    "            ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly Calibrated\")\n",
    "            ax.plot(prob_pred, prob_true, \"o-\", label=f\"Model (ECE={ece:.3f})\")\n",
    "            ax.fill_between(prob_pred, prob_pred, prob_true, alpha=0.2)\n",
    "            ax.set_xlabel(\"Mean Predicted Probability\")\n",
    "            ax.set_ylabel(\"Fraction of Positives\")\n",
    "            ax.set_title(f\"{pred_file.stem}\")\n",
    "            ax.legend(loc=\"lower right\")\n",
    "            ax.set_xlim([0, 1])\n",
    "            ax.set_ylim([0, 1])\n",
    "            \n",
    "            calibration_results.append({\n",
    "                \"Model\": pred_file.stem,\n",
    "                \"ECE\": ece,\n",
    "                \"Brier\": ((y_prob - y_true) ** 2).mean()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pred_file}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_DIR / \"calibration_reliability.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print calibration summary\n",
    "    if calibration_results:\n",
    "        cal_df = pd.DataFrame(calibration_results)\n",
    "        print(\"\\nCalibration Metrics:\")\n",
    "        print(cal_df.to_string(index=False))\n",
    "        cal_df.to_csv(EXPERIMENTS_DIR / \"calibration_metrics.csv\", index=False)\n",
    "        print(f\"\\nSaved: {EXPERIMENTS_DIR / 'calibration_reliability.png'}\")\n",
    "        print(f\"Saved: {EXPERIMENTS_DIR / 'calibration_metrics.csv'}\")\n",
    "else:\n",
    "    print(\"No prediction files found. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Fairness Metrics\n",
    "RUN_FAIRNESS = True # @param {type:\"boolean\"}\n",
    "\n",
    "# Define which datasets have identity labels and which split to use\n",
    "# NOTE: Only Jigsaw has g_* identity columns in *_full.csv files\n",
    "# Civil Comments full files only have 'toxicity' column, no identity groups\n",
    "fairness_targets = [\n",
    "    {\"dataset\": \"jigsaw\", \"split\": \"test\", \"full_file\": \"jigsaw_test_full.csv\", \"group_prefix\": \"g_\"},\n",
    "]\n",
    "\n",
    "if RUN_FAIRNESS:\n",
    "    print(\"Running Fairness Analysis...\")\n",
    "    print(\"Note: Only Jigsaw has identity group columns for fairness analysis.\\n\")\n",
    "    \n",
    "    for target in fairness_targets:\n",
    "        dataset = target[\"dataset\"]\n",
    "        split = target[\"split\"]\n",
    "        full_file = DATA_DIR / target[\"full_file\"]\n",
    "        \n",
    "        if not full_file.exists():\n",
    "            print(f\"Skipping {dataset}: Full data file {full_file} not found.\")\n",
    "            continue\n",
    "            \n",
    "        # Check for predictions from RoBERTa (in-domain and cross-domain)\n",
    "        # We look for preds_{source}_{dataset}.csv where dataset is the target\n",
    "        # For in-domain: preds_{dataset}_test.csv\n",
    "        \n",
    "        # 1. In-domain RoBERTa\n",
    "        pred_file = EXPERIMENTS_DIR / f\"preds_{dataset}_test.csv\"\n",
    "        if pred_file.exists():\n",
    "            print(f\"\\n--- Fairness: {dataset} (RoBERTa In-domain) ---\")\n",
    "            out_prefix = EXPERIMENTS_DIR / f\"fairness_{dataset}_roberta_indomain\"\n",
    "            cmd = [\n",
    "                sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                \"--dataset\", dataset,\n",
    "                \"--split\", split,\n",
    "                \"--pred_file\", str(pred_file),\n",
    "                \"--full_data_file\", str(full_file),\n",
    "                \"--group_prefix\", target[\"group_prefix\"],\n",
    "                \"--out_prefix\", str(out_prefix)\n",
    "            ]\n",
    "            subprocess.check_call(cmd)\n",
    "        else:\n",
    "            print(f\"RoBERTa predictions not found for {dataset} in-domain: {pred_file}\")\n",
    "\n",
    "        # 2. In-domain TF-IDF (LogReg)\n",
    "        pred_file = EXPERIMENTS_DIR / f\"preds_tfidf_logreg_{dataset}_test.csv\"\n",
    "        if pred_file.exists():\n",
    "            print(f\"\\n--- Fairness: {dataset} (TF-IDF LogReg In-domain) ---\")\n",
    "            out_prefix = EXPERIMENTS_DIR / f\"fairness_{dataset}_tfidf_logreg_indomain\"\n",
    "            cmd = [\n",
    "                sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                \"--dataset\", dataset,\n",
    "                \"--split\", split,\n",
    "                \"--pred_file\", str(pred_file),\n",
    "                \"--full_data_file\", str(full_file),\n",
    "                \"--group_prefix\", target[\"group_prefix\"],\n",
    "                \"--out_prefix\", str(out_prefix)\n",
    "            ]\n",
    "            subprocess.check_call(cmd)\n",
    "\n",
    "        # 3. Cross-domain (e.g., Civil -> Jigsaw, HateXplain -> Jigsaw)\n",
    "        for source in datasets:\n",
    "            if source == dataset: \n",
    "                continue\n",
    "            \n",
    "            # RoBERTa cross-domain\n",
    "            pred_file = EXPERIMENTS_DIR / f\"preds_{source}_to_{dataset}.csv\"\n",
    "            if pred_file.exists():\n",
    "                print(f\"\\n--- Fairness: {source} -> {dataset} (RoBERTa) ---\")\n",
    "                out_prefix = EXPERIMENTS_DIR / f\"fairness_{source}_to_{dataset}_roberta\"\n",
    "                cmd = [\n",
    "                    sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                    \"--dataset\", dataset,\n",
    "                    \"--split\", split,\n",
    "                    \"--pred_file\", str(pred_file),\n",
    "                    \"--full_data_file\", str(full_file),\n",
    "                    \"--group_prefix\", target[\"group_prefix\"],\n",
    "                    \"--out_prefix\", str(out_prefix)\n",
    "                ]\n",
    "                subprocess.check_call(cmd)\n",
    "            \n",
    "            # TF-IDF cross-domain\n",
    "            pred_file = EXPERIMENTS_DIR / f\"preds_tfidf_logreg_{source}_to_{dataset}.csv\"\n",
    "            if pred_file.exists():\n",
    "                print(f\"\\n--- Fairness: {source} -> {dataset} (TF-IDF LogReg) ---\")\n",
    "                out_prefix = EXPERIMENTS_DIR / f\"fairness_{source}_to_{dataset}_tfidf_logreg\"\n",
    "                cmd = [\n",
    "                    sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                    \"--dataset\", dataset,\n",
    "                    \"--split\", split,\n",
    "                    \"--pred_file\", str(pred_file),\n",
    "                    \"--full_data_file\", str(full_file),\n",
    "                    \"--group_prefix\", target[\"group_prefix\"],\n",
    "                    \"--out_prefix\", str(out_prefix)\n",
    "                ]\n",
    "                subprocess.check_call(cmd)\n",
    "            \n",
    "    print(\"\\nFairness Analysis Completed.\")\n",
    "else:\n",
    "    print(\"Skipping Fairness Analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ebd42",
   "metadata": {},
   "source": [
    "## 6. Results & Analysis\n",
    "\n",
    "We aggregate results from all experiments and visualize:\n",
    "1.  **Cross-Domain Performance**: F1 Score Heatmap.\n",
    "2.  **Fairness Gaps**: Demographic Parity and Equal Opportunity differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d026ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Aggregate Results\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load TF-IDF Summaries\n",
    "tfidf_files = glob.glob(str(EXPERIMENTS_DIR / \"summary_tfidf_*.csv\"))\n",
    "roberta_files = glob.glob(str(EXPERIMENTS_DIR / \"summary_*.csv\"))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# Process TF-IDF\n",
    "for f in tfidf_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        # Filename: summary_tfidf_{source}_{model}.csv\n",
    "        fname = Path(f).stem\n",
    "        parts = fname.split(\"_\")\n",
    "        # parts: ['summary', 'tfidf', source, model]\n",
    "        if len(parts) >= 4:\n",
    "            source = parts[2]\n",
    "            model = parts[3]\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        for _, row in df.iterrows():\n",
    "            split = row[\"split\"]\n",
    "            if split == \"in_domain_test\":\n",
    "                target = source\n",
    "            elif split.startswith(\"cross_\"):\n",
    "                target = split.replace(\"cross_\", \"\")\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            all_metrics.append({\n",
    "                \"Model\": f\"TF-IDF ({model})\",\n",
    "                \"Source\": source,\n",
    "                \"Target\": target,\n",
    "                \"F1\": row.get(\"f1\", 0),\n",
    "                \"Accuracy\": row.get(\"accuracy\", 0),\n",
    "                \"AUROC\": row.get(\"auroc\", 0)\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "# Process RoBERTa\n",
    "for f in roberta_files:\n",
    "    if \"tfidf\" in f: continue # Skip tfidf here\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        source = Path(f).stem.replace(\"summary_\", \"\")\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            split = row[\"split\"]\n",
    "            if split == \"in_domain_test\":\n",
    "                target = source\n",
    "            elif split.startswith(\"cross_\"):\n",
    "                target = split.replace(\"cross_\", \"\")\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            all_metrics.append({\n",
    "                \"Model\": \"RoBERTa\",\n",
    "                \"Source\": source,\n",
    "                \"Target\": target,\n",
    "                \"F1\": row.get(\"f1\", 0),\n",
    "                \"Accuracy\": row.get(\"accuracy\", 0),\n",
    "                \"AUROC\": row.get(\"auroc\", 0)\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"Loaded Results:\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b98fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot Cross-Domain Performance Heatmaps\n",
    "if not results_df.empty:\n",
    "    models = results_df[\"Model\"].unique()\n",
    "    n_models = len(models)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, model in zip(axes, models):\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        if model_data.empty:\n",
    "            continue\n",
    "        pivot_table = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        sns.heatmap(pivot_table, annot=True, cmap=\"viridis\", fmt=\".3f\", ax=ax, vmin=0, vmax=1)\n",
    "        ax.set_title(f\"{model} Cross-Domain F1 Score\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_DIR / \"cross_domain_f1_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {EXPERIMENTS_DIR / 'cross_domain_f1_heatmap.png'}\")\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b663a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot ROC Curves Comparison\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Find all prediction files for ROC curves\n",
    "all_pred_files = list(EXPERIMENTS_DIR.glob(\"preds_*.csv\"))\n",
    "\n",
    "if all_pred_files:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Group by target dataset\n",
    "    for idx, target_dataset in enumerate([\"civil\", \"hatexplain\", \"jigsaw\"]):\n",
    "        ax = axes[idx]\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "        \n",
    "        # Find all predictions that evaluate on this target\n",
    "        for pred_file in all_pred_files:\n",
    "            fname = pred_file.stem\n",
    "            \n",
    "            # Check if this file evaluates on target_dataset\n",
    "            is_match = False\n",
    "            if f\"_to_{target_dataset}\" in fname:\n",
    "                is_match = True\n",
    "                label = fname.replace(\"preds_\", \"\").replace(f\"_to_{target_dataset}\", f\"→{target_dataset}\")\n",
    "            elif fname == f\"preds_{target_dataset}_test\":\n",
    "                is_match = True\n",
    "                label = f\"{target_dataset} (in-domain)\"\n",
    "            elif fname == f\"preds_tfidf_logreg_{target_dataset}_test\":\n",
    "                is_match = True\n",
    "                label = f\"TF-IDF {target_dataset} (in-domain)\"\n",
    "            \n",
    "            if not is_match:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(pred_file)\n",
    "                prob_col = \"pos_prob\" if \"pos_prob\" in df.columns else \"score\"\n",
    "                if prob_col not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                y_true = df[\"label\"].values\n",
    "                y_score = df[prob_col].values\n",
    "                \n",
    "                fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                ax.plot(fpr, tpr, lw=2, label=f'{label} (AUC={roc_auc:.3f})')\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {pred_file}: {e}\")\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(f'ROC Curves: Target = {target_dataset.upper()}')\n",
    "        ax.legend(loc='lower right', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_DIR / \"roc_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {EXPERIMENTS_DIR / 'roc_curves.png'}\")\n",
    "else:\n",
    "    print(\"No prediction files found for ROC curves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot Fairness Gaps\n",
    "fairness_files = glob.glob(str(EXPERIMENTS_DIR / \"fairness_*_summary.csv\"))\n",
    "\n",
    "fairness_data = []\n",
    "\n",
    "for f in fairness_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        # Filename: fairness_{context}_summary.csv\n",
    "        fname = Path(f).stem\n",
    "        context = fname.replace(\"fairness_\", \"\").replace(\"_summary\", \"\")\n",
    "        \n",
    "        # Get max gaps across all identity groups\n",
    "        max_dp = df[\"dp_diff\"].max()\n",
    "        max_eop = df[\"eop_diff\"].max()\n",
    "        max_eo = df[\"eo_diff\"].max()\n",
    "        \n",
    "        fairness_data.append({\n",
    "            \"Setting\": context,\n",
    "            \"Max DP Gap\": max_dp,\n",
    "            \"Max EOp Gap\": max_eop,\n",
    "            \"Max EO Gap\": max_eo\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "if fairness_data:\n",
    "    f_df = pd.DataFrame(fairness_data)\n",
    "    print(\"\\nFairness Gaps Summary:\")\n",
    "    print(f_df.to_string(index=False))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    f_df.set_index(\"Setting\")[[\"Max DP Gap\", \"Max EOp Gap\", \"Max EO Gap\"]].plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(\"Fairness Gaps by Setting (Lower is Better)\")\n",
    "    ax.set_ylabel(\"Gap\")\n",
    "    ax.set_xlabel(\"Model / Domain Setting\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(title=\"Metric\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_DIR / \"fairness_gaps.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {EXPERIMENTS_DIR / 'fairness_gaps.png'}\")\n",
    "else:\n",
    "    print(\"No fairness results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fe5e7",
   "metadata": {},
   "source": [
    "## 7. Summary Tables for Report\n",
    "\n",
    "Generate formatted tables ready for the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e86999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate Summary Tables\n",
    "if not results_df.empty:\n",
    "    # Table 1: In-Domain Performance\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TABLE 1: In-Domain Performance (Source = Target)\")\n",
    "    print(\"=\" * 80)\n",
    "    in_domain = results_df[results_df[\"Source\"] == results_df[\"Target\"]]\n",
    "    in_domain_pivot = in_domain.pivot(index=\"Source\", columns=\"Model\", values=[\"F1\", \"Accuracy\", \"AUROC\"])\n",
    "    print(in_domain_pivot.round(4).to_string())\n",
    "    \n",
    "    # Table 2: Cross-Domain Performance (F1)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TABLE 2: Cross-Domain F1 Score\")\n",
    "    print(\"=\" * 80)\n",
    "    for model in results_df[\"Model\"].unique():\n",
    "        print(f\"\\n{model}:\")\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        pivot = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        print(pivot.round(4).to_string())\n",
    "    \n",
    "    # Table 3: OOD Performance Drop\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TABLE 3: OOD Performance Drop (In-Domain F1 - Cross-Domain F1)\")\n",
    "    print(\"=\" * 80)\n",
    "    for model in results_df[\"Model\"].unique():\n",
    "        print(f\"\\n{model}:\")\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        pivot = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        # Calculate drop from diagonal (in-domain)\n",
    "        for src in pivot.index:\n",
    "            if src in pivot.columns:\n",
    "                in_domain_f1 = pivot.loc[src, src]\n",
    "                for tgt in pivot.columns:\n",
    "                    if src != tgt:\n",
    "                        pivot.loc[src, tgt] = in_domain_f1 - pivot.loc[src, tgt]\n",
    "                pivot.loc[src, src] = 0  # No drop for in-domain\n",
    "        print(pivot.round(4).to_string())\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(EXPERIMENTS_DIR / \"all_results_combined.csv\", index=False)\n",
    "    print(f\"\\nSaved all results to: {EXPERIMENTS_DIR / 'all_results_combined.csv'}\")\n",
    "else:\n",
    "    print(\"No results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Export Results for Report (LaTeX Tables)\n",
    "def df_to_latex(df, caption=\"\", label=\"\"):\n",
    "    \"\"\"Convert DataFrame to LaTeX table format.\"\"\"\n",
    "    latex = df.to_latex(float_format=\"%.4f\", escape=False)\n",
    "    if caption:\n",
    "        latex = latex.replace(\"\\\\begin{tabular}\", f\"\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\\\\begin{tabular}\")\n",
    "    return latex\n",
    "\n",
    "if not results_df.empty:\n",
    "    # Export main results table\n",
    "    for model in results_df[\"Model\"].unique():\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        pivot = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        \n",
    "        model_name_clean = model.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        latex_file = EXPERIMENTS_DIR / f\"table_{model_name_clean}_f1.tex\"\n",
    "        \n",
    "        with open(latex_file, \"w\") as f:\n",
    "            f.write(df_to_latex(pivot, \n",
    "                               caption=f\"{model} Cross-Domain F1 Scores\",\n",
    "                               label=f\"tab:{model_name_clean}_f1\"))\n",
    "        print(f\"Saved LaTeX table: {latex_file}\")\n",
    "\n",
    "# Export fairness table\n",
    "if fairness_data:\n",
    "    f_df = pd.DataFrame(fairness_data)\n",
    "    latex_file = EXPERIMENTS_DIR / \"table_fairness.tex\"\n",
    "    with open(latex_file, \"w\") as f:\n",
    "        f.write(df_to_latex(f_df.set_index(\"Setting\"), \n",
    "                           caption=\"Fairness Gaps Across Models and Domains\",\n",
    "                           label=\"tab:fairness\"))\n",
    "    print(f\"Saved LaTeX table: {latex_file}\")\n",
    "\n",
    "print(\"\\n✓ All exports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200cba6",
   "metadata": {},
   "source": [
    "## 8. Output Verification & Checklist\n",
    "\n",
    "Verify all expected outputs were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0260ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Verify All Outputs\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTPUT VERIFICATION CHECKLIST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "expected_outputs = {\n",
    "    \"Plots\": [\n",
    "        \"eda_plots.png\",\n",
    "        \"cross_domain_f1_heatmap.png\",\n",
    "        \"roc_curves.png\",\n",
    "        \"calibration_reliability.png\",\n",
    "        \"fairness_gaps.png\",\n",
    "    ],\n",
    "    \"Tables (CSV)\": [\n",
    "        \"all_results_combined.csv\",\n",
    "        \"calibration_metrics.csv\",\n",
    "    ],\n",
    "    \"LaTeX Tables\": [\n",
    "        \"table_RoBERTa_f1.tex\",\n",
    "        \"table_TF-IDF_logreg_f1.tex\",\n",
    "        \"table_TF-IDF_svm_f1.tex\",\n",
    "        \"table_fairness.tex\",\n",
    "    ],\n",
    "    \"TF-IDF Summaries\": [\n",
    "        \"summary_tfidf_civil_logreg.csv\",\n",
    "        \"summary_tfidf_civil_svm.csv\",\n",
    "        \"summary_tfidf_hatexplain_logreg.csv\",\n",
    "        \"summary_tfidf_hatexplain_svm.csv\",\n",
    "        \"summary_tfidf_jigsaw_logreg.csv\",\n",
    "        \"summary_tfidf_jigsaw_svm.csv\",\n",
    "    ],\n",
    "    \"RoBERTa Summaries\": [\n",
    "        \"summary_civil.csv\",\n",
    "        \"summary_hatexplain.csv\",\n",
    "        \"summary_jigsaw.csv\",\n",
    "    ],\n",
    "    \"Fairness Results\": [\n",
    "        \"fairness_jigsaw_roberta_indomain_summary.csv\",\n",
    "        \"fairness_jigsaw_tfidf_logreg_indomain_summary.csv\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "total_expected = 0\n",
    "total_found = 0\n",
    "\n",
    "for category, files in expected_outputs.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for f in files:\n",
    "        total_expected += 1\n",
    "        path = EXPERIMENTS_DIR / f\n",
    "        if path.exists():\n",
    "            print(f\"  ✓ {f}\")\n",
    "            total_found += 1\n",
    "        else:\n",
    "            print(f\"  ✗ {f} (MISSING)\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"SUMMARY: {total_found}/{total_expected} files generated\")\n",
    "if total_found == total_expected:\n",
    "    print(\"✓ All outputs verified!\")\n",
    "else:\n",
    "    print(f\"⚠️  {total_expected - total_found} files missing. Re-run relevant cells.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66fd94",
   "metadata": {},
   "source": [
    "## 9. Usage Notes\n",
    "\n",
    "### Running on Google Colab Pro\n",
    "1. Upload the entire repository to Colab or mount from Google Drive\n",
    "2. Ensure GPU runtime is enabled: `Runtime → Change runtime type → GPU`\n",
    "3. Run cells in order, adjusting toggles:\n",
    "   - `RUN_TFIDF = True` → Runs TF-IDF baselines (~5-10 min)\n",
    "   - `RUN_ROBERTA = True` → Runs RoBERTa experiments (~30-60 min per dataset with GPU)\n",
    "   - `FAST_MODE = True` → Uses 1 epoch for quick testing\n",
    "   - `RUN_FAIRNESS = True` → Computes fairness metrics on Jigsaw dataset\n",
    "\n",
    "### Expected Outputs for Report\n",
    "| Output Type | Files | Description |\n",
    "|-------------|-------|-------------|\n",
    "| **EDA** | `eda_plots.png` | Dataset statistics, label distributions |\n",
    "| **Performance** | `cross_domain_f1_heatmap.png` | Cross-domain F1 score matrix |\n",
    "| **ROC Curves** | `roc_curves.png` | Model comparison by AUC |\n",
    "| **Calibration** | `calibration_reliability.png`, `calibration_metrics.csv` | ECE, Brier scores |\n",
    "| **Fairness** | `fairness_gaps.png`, `fairness_*_summary.csv` | DP, EOp, EO gaps |\n",
    "| **Tables** | `table_*.tex` | LaTeX-ready tables |\n",
    "| **Raw Data** | `all_results_combined.csv` | All metrics in one file |\n",
    "\n",
    "### Key Findings to Report\n",
    "1. **In-Domain Performance**: How well models perform on their training distribution\n",
    "2. **Cross-Domain Generalization**: Performance drop when tested on different datasets  \n",
    "3. **Calibration**: Model confidence vs actual accuracy (ECE, Brier)\n",
    "4. **Fairness**: Demographic parity and equal opportunity gaps across identity groups"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
