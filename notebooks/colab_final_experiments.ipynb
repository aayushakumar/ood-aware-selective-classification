{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f178439",
   "metadata": {},
   "source": [
    "# OOD Evaluation of Toxic Comment Classifiers\n",
    "## Final Project Experiments & Results\n",
    "\n",
    "This notebook reproduces all experiments, results, and plots for the project.\n",
    "It covers:\n",
    "1.  **Setup**: Environment and Data.\n",
    "2.  **Baselines**: TF-IDF + Logistic Regression / SVM.\n",
    "3.  **Models**: RoBERTa (In-domain and Cross-domain).\n",
    "4.  **Fairness**: Demographic Parity and Equal Opportunity analysis.\n",
    "5.  **Analysis**: Final plots and tables.\n",
    "\n",
    "**Datasets**: Civil Comments, HateXplain, Jigsaw.\n",
    "**Goal**: Evaluate OOD generalization and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Setup & Installation\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running in Local Environment\")\n",
    "\n",
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    print(\"Installing dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    # Install specific versions if needed, e.g. transformers\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"scikit-learn\", \"pandas\", \"matplotlib\", \"seaborn\", \"torch\"])\n",
    "\n",
    "# Mount Drive if needed (optional)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "REPO_DIR = Path(os.getcwd())\n",
    "if IN_COLAB:\n",
    "    # Assuming repo is cloned to /content/ood-eval-toxic-classifiers or similar\n",
    "    # If not, clone it:\n",
    "    if not (REPO_DIR / \"scripts\").exists():\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/aayushakumar/ood-eval-toxic-classifiers.git\n",
    "        os.chdir(\"ood-eval-toxic-classifiers\")\n",
    "        REPO_DIR = Path(os.getcwd())\n",
    "\n",
    "DATA_DIR = REPO_DIR / \"data\"\n",
    "EXPERIMENTS_DIR = REPO_DIR / \"experiments\"\n",
    "SCRIPTS_DIR = REPO_DIR / \"scripts\"\n",
    "\n",
    "EXPERIMENTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Working Directory: {REPO_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Experiments Directory: {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c05acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Data Verification\n",
    "import pandas as pd\n",
    "\n",
    "REQUIRED_FILES = [\n",
    "    \"civil_train.csv\", \"civil_val.csv\", \"civil_test.csv\",\n",
    "    \"hatexplain_train.csv\", \"hatexplain_val.csv\", \"hatexplain_test.csv\",\n",
    "    \"jigsaw_train.csv\", \"jigsaw_val.csv\", \"jigsaw_test.csv\"\n",
    "]\n",
    "\n",
    "FULL_FILES = [\n",
    "    \"jigsaw_test_full.csv\",  # Required for fairness analysis\n",
    "    \"jigsaw_train_full.csv\",\n",
    "    \"jigsaw_val_full.csv\",\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for fname in REQUIRED_FILES:\n",
    "    if not (DATA_DIR / fname).exists():\n",
    "        missing_files.append(fname)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"❌ WARNING: Missing data files: {missing_files}\")\n",
    "    print(\"Please ensure data is uploaded to the 'data/' directory.\")\n",
    "else:\n",
    "    print(\"✓ All required data files found.\")\n",
    "\n",
    "# Check full files for fairness\n",
    "missing_full = []\n",
    "for fname in FULL_FILES:\n",
    "    if not (DATA_DIR / fname).exists():\n",
    "        missing_full.append(fname)\n",
    "\n",
    "if missing_full:\n",
    "    print(f\"\\n⚠️  Missing full data files (needed for fairness): {missing_full}\")\n",
    "else:\n",
    "    print(\"✓ Full data files for fairness analysis found.\")\n",
    "\n",
    "# Dataset statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset in [\"civil\", \"hatexplain\", \"jigsaw\"]:\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        fpath = DATA_DIR / f\"{dataset}_{split}.csv\"\n",
    "        if fpath.exists():\n",
    "            df = pd.read_csv(fpath)\n",
    "            n_toxic = (df[\"label\"] == 1).sum()\n",
    "            n_total = len(df)\n",
    "            print(f\"  {split:5s}: {n_total:6d} samples, {n_toxic:5d} toxic ({100*n_toxic/n_total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827c564",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Baselines\n",
    "\n",
    "We train Logistic Regression and Linear SVM models on each source dataset and evaluate on all target datasets.\n",
    "Metrics: Accuracy, F1, ROC-AUC, PR-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ccaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run TF-IDF Experiments\n",
    "RUN_TFIDF = True # @param {type:\"boolean\"}\n",
    "\n",
    "datasets = [\"civil\", \"hatexplain\", \"jigsaw\"]\n",
    "\n",
    "if RUN_TFIDF:\n",
    "    print(\"Running TF-IDF Baselines...\")\n",
    "    for source in datasets:\n",
    "        # Target all other datasets + self\n",
    "        targets = [d for d in datasets]\n",
    "        \n",
    "        print(f\"\\n--- Training on {source} ---\")\n",
    "        cmd = [\n",
    "            sys.executable, \"scripts/run_tfidf_baselines.py\",\n",
    "            \"--source_dataset\", source,\n",
    "            \"--target_datasets\"\n",
    "        ] + targets + [\n",
    "            \"--model\", \"both\",  # Run both LogReg and SVM\n",
    "            \"--save_preds\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"Executing: {' '.join(cmd)}\")\n",
    "        subprocess.check_call(cmd)\n",
    "        \n",
    "    print(\"\\nTF-IDF Experiments Completed.\")\n",
    "else:\n",
    "    print(\"Skipping TF-IDF experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef3a65",
   "metadata": {},
   "source": [
    "## 4. RoBERTa Experiments\n",
    "\n",
    "We train RoBERTa-base models.\n",
    "Options:\n",
    "- **Standard**: Fine-tuning on source.\n",
    "- **CORAL**: Domain adaptation (requires unlabeled target).\n",
    "- **LoRA**: Parameter-efficient fine-tuning.\n",
    "\n",
    "We also enable **Calibration** (Temperature Scaling) and save predictions for fairness analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run RoBERTa Experiments\n",
    "RUN_ROBERTA = True # @param {type:\"boolean\"}\n",
    "USE_CORAL = False # @param {type:\"boolean\"}\n",
    "USE_LORA = False # @param {type:\"boolean\"}\n",
    "FAST_MODE = True # @param {type:\"boolean\"}\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 1 if FAST_MODE else 3\n",
    "SEEDS = [42] # Add more seeds for full paper results, e.g. [42, 123, 456]\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "if RUN_ROBERTA:\n",
    "    print(\"Running RoBERTa Experiments...\")\n",
    "    \n",
    "    for source in datasets:\n",
    "        targets = [d for d in datasets] # All datasets\n",
    "        \n",
    "        # Base arguments\n",
    "        cmd = [\n",
    "            sys.executable, \"scripts/run_roberta.py\",\n",
    "            \"--source_dataset\", source,\n",
    "            \"--model_name\", \"roberta-base\",\n",
    "            \"--epochs\", str(EPOCHS),\n",
    "            \"--batch_size\", str(BATCH_SIZE),\n",
    "            \"--max_len\", str(MAX_LEN),\n",
    "            \"--seeds\"\n",
    "        ] + [str(s) for s in SEEDS] + [\n",
    "            \"--target_datasets\"\n",
    "        ] + targets + [\n",
    "            \"--calibration\", \"temperature\", # Enable calibration\n",
    "            \"--save_preds\",\n",
    "            \"--amp\", # Use Mixed Precision\n",
    "            \"--tune_threshold\"\n",
    "        ]\n",
    "        \n",
    "        # LoRA\n",
    "        if USE_LORA:\n",
    "            cmd += [\"--peft\", \"lora\"]\n",
    "            \n",
    "        # CORAL (Example: if source is civil, adapt to jigsaw)\n",
    "        # Note: CORAL requires a specific target. For simplicity, we run standard here.\n",
    "        # To run CORAL, you would need a separate loop or logic.\n",
    "        \n",
    "        print(f\"\\n--- Training RoBERTa on {source} ---\")\n",
    "        print(f\"Executing: {' '.join(cmd)}\")\n",
    "        \n",
    "        # Run\n",
    "        subprocess.check_call(cmd)\n",
    "        \n",
    "    print(\"\\nRoBERTa Experiments Completed.\")\n",
    "else:\n",
    "    print(\"Skipping RoBERTa experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382476e",
   "metadata": {},
   "source": [
    "## 5. Fairness Analysis\n",
    "\n",
    "We compute fairness metrics (Demographic Parity, Equal Opportunity) for models on datasets with identity attributes (Jigsaw, Civil).\n",
    "We use the `scripts/fairness_metrics.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Fairness Metrics\n",
    "RUN_FAIRNESS = True # @param {type:\"boolean\"}\n",
    "\n",
    "# Define which datasets have identity labels and which split to use\n",
    "# NOTE: Only Jigsaw has g_* identity columns in *_full.csv files\n",
    "# Civil Comments full files only have 'toxicity' column, no identity groups\n",
    "fairness_targets = [\n",
    "    {\"dataset\": \"jigsaw\", \"split\": \"test\", \"full_file\": \"jigsaw_test_full.csv\", \"group_prefix\": \"g_\"},\n",
    "]\n",
    "\n",
    "if RUN_FAIRNESS:\n",
    "    print(\"Running Fairness Analysis...\")\n",
    "    print(\"Note: Only Jigsaw has identity group columns for fairness analysis.\\n\")\n",
    "    \n",
    "    for target in fairness_targets:\n",
    "        dataset = target[\"dataset\"]\n",
    "        split = target[\"split\"]\n",
    "        full_file = DATA_DIR / target[\"full_file\"]\n",
    "        \n",
    "        if not full_file.exists():\n",
    "            print(f\"Skipping {dataset}: Full data file {full_file} not found.\")\n",
    "            continue\n",
    "            \n",
    "        # Check for predictions from RoBERTa (in-domain and cross-domain)\n",
    "        # We look for preds_{source}_{dataset}.csv where dataset is the target\n",
    "        # For in-domain: preds_{dataset}_test.csv\n",
    "        \n",
    "        # 1. In-domain RoBERTa\n",
    "        pred_file = EXPERIMENTS_DIR / f\"preds_{dataset}_test.csv\"\n",
    "        if pred_file.exists():\n",
    "            print(f\"\\n--- Fairness: {dataset} (RoBERTa In-domain) ---\")\n",
    "            out_prefix = EXPERIMENTS_DIR / f\"fairness_{dataset}_roberta_indomain\"\n",
    "            cmd = [\n",
    "                sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                \"--dataset\", dataset,\n",
    "                \"--split\", split,\n",
    "                \"--pred_file\", str(pred_file),\n",
    "                \"--full_data_file\", str(full_file),\n",
    "                \"--group_prefix\", target[\"group_prefix\"],\n",
    "                \"--out_prefix\", str(out_prefix)\n",
    "            ]\n",
    "            subprocess.check_call(cmd)\n",
    "        else:\n",
    "            print(f\"RoBERTa predictions not found for {dataset} in-domain: {pred_file}\")\n",
    "\n",
    "        # 2. In-domain TF-IDF (LogReg)\n",
    "        pred_file = EXPERIMENTS_DIR / f\"preds_tfidf_logreg_{dataset}_test.csv\"\n",
    "        if pred_file.exists():\n",
    "            print(f\"\\n--- Fairness: {dataset} (TF-IDF LogReg In-domain) ---\")\n",
    "            out_prefix = EXPERIMENTS_DIR / f\"fairness_{dataset}_tfidf_logreg_indomain\"\n",
    "            cmd = [\n",
    "                sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                \"--dataset\", dataset,\n",
    "                \"--split\", split,\n",
    "                \"--pred_file\", str(pred_file),\n",
    "                \"--full_data_file\", str(full_file),\n",
    "                \"--group_prefix\", target[\"group_prefix\"],\n",
    "                \"--out_prefix\", str(out_prefix)\n",
    "            ]\n",
    "            subprocess.check_call(cmd)\n",
    "\n",
    "        # 3. Cross-domain (e.g., Civil -> Jigsaw, HateXplain -> Jigsaw)\n",
    "        for source in datasets:\n",
    "            if source == dataset: \n",
    "                continue\n",
    "            \n",
    "            # RoBERTa cross-domain\n",
    "            pred_file = EXPERIMENTS_DIR / f\"preds_{source}_to_{dataset}.csv\"\n",
    "            if pred_file.exists():\n",
    "                print(f\"\\n--- Fairness: {source} -> {dataset} (RoBERTa) ---\")\n",
    "                out_prefix = EXPERIMENTS_DIR / f\"fairness_{source}_to_{dataset}_roberta\"\n",
    "                cmd = [\n",
    "                    sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                    \"--dataset\", dataset,\n",
    "                    \"--split\", split,\n",
    "                    \"--pred_file\", str(pred_file),\n",
    "                    \"--full_data_file\", str(full_file),\n",
    "                    \"--group_prefix\", target[\"group_prefix\"],\n",
    "                    \"--out_prefix\", str(out_prefix)\n",
    "                ]\n",
    "                subprocess.check_call(cmd)\n",
    "            \n",
    "            # TF-IDF cross-domain\n",
    "            pred_file = EXPERIMENTS_DIR / f\"preds_tfidf_logreg_{source}_to_{dataset}.csv\"\n",
    "            if pred_file.exists():\n",
    "                print(f\"\\n--- Fairness: {source} -> {dataset} (TF-IDF LogReg) ---\")\n",
    "                out_prefix = EXPERIMENTS_DIR / f\"fairness_{source}_to_{dataset}_tfidf_logreg\"\n",
    "                cmd = [\n",
    "                    sys.executable, \"scripts/fairness_metrics.py\",\n",
    "                    \"--dataset\", dataset,\n",
    "                    \"--split\", split,\n",
    "                    \"--pred_file\", str(pred_file),\n",
    "                    \"--full_data_file\", str(full_file),\n",
    "                    \"--group_prefix\", target[\"group_prefix\"],\n",
    "                    \"--out_prefix\", str(out_prefix)\n",
    "                ]\n",
    "                subprocess.check_call(cmd)\n",
    "            \n",
    "    print(\"\\nFairness Analysis Completed.\")\n",
    "else:\n",
    "    print(\"Skipping Fairness Analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ebd42",
   "metadata": {},
   "source": [
    "## 6. Results & Analysis\n",
    "\n",
    "We aggregate results from all experiments and visualize:\n",
    "1.  **Cross-Domain Performance**: F1 Score Heatmap.\n",
    "2.  **Fairness Gaps**: Demographic Parity and Equal Opportunity differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d026ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Aggregate Results\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load TF-IDF Summaries\n",
    "tfidf_files = glob.glob(str(EXPERIMENTS_DIR / \"summary_tfidf_*.csv\"))\n",
    "roberta_files = glob.glob(str(EXPERIMENTS_DIR / \"summary_*.csv\"))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# Process TF-IDF\n",
    "for f in tfidf_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        # Filename: summary_tfidf_{source}_{model}.csv\n",
    "        fname = Path(f).stem\n",
    "        parts = fname.split(\"_\")\n",
    "        # parts: ['summary', 'tfidf', source, model]\n",
    "        if len(parts) >= 4:\n",
    "            source = parts[2]\n",
    "            model = parts[3]\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        for _, row in df.iterrows():\n",
    "            split = row[\"split\"]\n",
    "            if split == \"in_domain_test\":\n",
    "                target = source\n",
    "            elif split.startswith(\"cross_\"):\n",
    "                target = split.replace(\"cross_\", \"\")\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            all_metrics.append({\n",
    "                \"Model\": f\"TF-IDF ({model})\",\n",
    "                \"Source\": source,\n",
    "                \"Target\": target,\n",
    "                \"F1\": row.get(\"f1\", 0),\n",
    "                \"Accuracy\": row.get(\"accuracy\", 0),\n",
    "                \"AUROC\": row.get(\"auroc\", 0)\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "# Process RoBERTa\n",
    "for f in roberta_files:\n",
    "    if \"tfidf\" in f: continue # Skip tfidf here\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        source = Path(f).stem.replace(\"summary_\", \"\")\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            split = row[\"split\"]\n",
    "            if split == \"in_domain_test\":\n",
    "                target = source\n",
    "            elif split.startswith(\"cross_\"):\n",
    "                target = split.replace(\"cross_\", \"\")\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            all_metrics.append({\n",
    "                \"Model\": \"RoBERTa\",\n",
    "                \"Source\": source,\n",
    "                \"Target\": target,\n",
    "                \"F1\": row.get(\"f1\", 0),\n",
    "                \"Accuracy\": row.get(\"accuracy\", 0),\n",
    "                \"AUROC\": row.get(\"auroc\", 0)\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"Loaded Results:\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b98fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot Cross-Domain Performance Heatmaps\n",
    "if not results_df.empty:\n",
    "    models = results_df[\"Model\"].unique()\n",
    "    n_models = len(models)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, model in zip(axes, models):\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        if model_data.empty:\n",
    "            continue\n",
    "        pivot_table = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        sns.heatmap(pivot_table, annot=True, cmap=\"viridis\", fmt=\".3f\", ax=ax, vmin=0, vmax=1)\n",
    "        ax.set_title(f\"{model} Cross-Domain F1 Score\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_DIR / \"cross_domain_f1_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {EXPERIMENTS_DIR / 'cross_domain_f1_heatmap.png'}\")\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot Fairness Gaps\n",
    "fairness_files = glob.glob(str(EXPERIMENTS_DIR / \"fairness_*_summary.csv\"))\n",
    "\n",
    "fairness_data = []\n",
    "\n",
    "for f in fairness_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        # Filename: fairness_{context}_summary.csv\n",
    "        fname = Path(f).stem\n",
    "        context = fname.replace(\"fairness_\", \"\").replace(\"_summary\", \"\")\n",
    "        \n",
    "        # Get max gaps across all identity groups\n",
    "        max_dp = df[\"dp_diff\"].max()\n",
    "        max_eop = df[\"eop_diff\"].max()\n",
    "        max_eo = df[\"eo_diff\"].max()\n",
    "        \n",
    "        fairness_data.append({\n",
    "            \"Setting\": context,\n",
    "            \"Max DP Gap\": max_dp,\n",
    "            \"Max EOp Gap\": max_eop,\n",
    "            \"Max EO Gap\": max_eo\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "if fairness_data:\n",
    "    f_df = pd.DataFrame(fairness_data)\n",
    "    print(\"\\nFairness Gaps Summary:\")\n",
    "    print(f_df.to_string(index=False))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    f_df.set_index(\"Setting\")[[\"Max DP Gap\", \"Max EOp Gap\", \"Max EO Gap\"]].plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(\"Fairness Gaps by Setting (Lower is Better)\")\n",
    "    ax.set_ylabel(\"Gap\")\n",
    "    ax.set_xlabel(\"Model / Domain Setting\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(title=\"Metric\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_DIR / \"fairness_gaps.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved: {EXPERIMENTS_DIR / 'fairness_gaps.png'}\")\n",
    "else:\n",
    "    print(\"No fairness results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fe5e7",
   "metadata": {},
   "source": [
    "## 7. Summary Tables for Report\n",
    "\n",
    "Generate formatted tables ready for the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e86999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate Summary Tables\n",
    "if not results_df.empty:\n",
    "    # Table 1: In-Domain Performance\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TABLE 1: In-Domain Performance (Source = Target)\")\n",
    "    print(\"=\" * 80)\n",
    "    in_domain = results_df[results_df[\"Source\"] == results_df[\"Target\"]]\n",
    "    in_domain_pivot = in_domain.pivot(index=\"Source\", columns=\"Model\", values=[\"F1\", \"Accuracy\", \"AUROC\"])\n",
    "    print(in_domain_pivot.round(4).to_string())\n",
    "    \n",
    "    # Table 2: Cross-Domain Performance (F1)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TABLE 2: Cross-Domain F1 Score\")\n",
    "    print(\"=\" * 80)\n",
    "    for model in results_df[\"Model\"].unique():\n",
    "        print(f\"\\n{model}:\")\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        pivot = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        print(pivot.round(4).to_string())\n",
    "    \n",
    "    # Table 3: OOD Performance Drop\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TABLE 3: OOD Performance Drop (In-Domain F1 - Cross-Domain F1)\")\n",
    "    print(\"=\" * 80)\n",
    "    for model in results_df[\"Model\"].unique():\n",
    "        print(f\"\\n{model}:\")\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        pivot = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        # Calculate drop from diagonal (in-domain)\n",
    "        for src in pivot.index:\n",
    "            if src in pivot.columns:\n",
    "                in_domain_f1 = pivot.loc[src, src]\n",
    "                for tgt in pivot.columns:\n",
    "                    if src != tgt:\n",
    "                        pivot.loc[src, tgt] = in_domain_f1 - pivot.loc[src, tgt]\n",
    "                pivot.loc[src, src] = 0  # No drop for in-domain\n",
    "        print(pivot.round(4).to_string())\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(EXPERIMENTS_DIR / \"all_results_combined.csv\", index=False)\n",
    "    print(f\"\\nSaved all results to: {EXPERIMENTS_DIR / 'all_results_combined.csv'}\")\n",
    "else:\n",
    "    print(\"No results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Export Results for Report (LaTeX Tables)\n",
    "def df_to_latex(df, caption=\"\", label=\"\"):\n",
    "    \"\"\"Convert DataFrame to LaTeX table format.\"\"\"\n",
    "    latex = df.to_latex(float_format=\"%.4f\", escape=False)\n",
    "    if caption:\n",
    "        latex = latex.replace(\"\\\\begin{tabular}\", f\"\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\\\\begin{tabular}\")\n",
    "    return latex\n",
    "\n",
    "if not results_df.empty:\n",
    "    # Export main results table\n",
    "    for model in results_df[\"Model\"].unique():\n",
    "        model_data = results_df[results_df[\"Model\"] == model]\n",
    "        pivot = model_data.pivot(index=\"Source\", columns=\"Target\", values=\"F1\")\n",
    "        \n",
    "        model_name_clean = model.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        latex_file = EXPERIMENTS_DIR / f\"table_{model_name_clean}_f1.tex\"\n",
    "        \n",
    "        with open(latex_file, \"w\") as f:\n",
    "            f.write(df_to_latex(pivot, \n",
    "                               caption=f\"{model} Cross-Domain F1 Scores\",\n",
    "                               label=f\"tab:{model_name_clean}_f1\"))\n",
    "        print(f\"Saved LaTeX table: {latex_file}\")\n",
    "\n",
    "# Export fairness table\n",
    "if fairness_data:\n",
    "    f_df = pd.DataFrame(fairness_data)\n",
    "    latex_file = EXPERIMENTS_DIR / \"table_fairness.tex\"\n",
    "    with open(latex_file, \"w\") as f:\n",
    "        f.write(df_to_latex(f_df.set_index(\"Setting\"), \n",
    "                           caption=\"Fairness Gaps Across Models and Domains\",\n",
    "                           label=\"tab:fairness\"))\n",
    "    print(f\"Saved LaTeX table: {latex_file}\")\n",
    "\n",
    "print(\"\\n✓ All exports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200cba6",
   "metadata": {},
   "source": [
    "## 8. Usage Notes\n",
    "\n",
    "### Running on Google Colab Pro\n",
    "1. Upload the entire repository to Colab or mount from Google Drive\n",
    "2. Ensure GPU runtime is enabled: `Runtime → Change runtime type → GPU`\n",
    "3. Run cells in order, adjusting toggles:\n",
    "   - `RUN_TFIDF = True` → Runs TF-IDF baselines (~5-10 min)\n",
    "   - `RUN_ROBERTA = True` → Runs RoBERTa experiments (~30-60 min per dataset with GPU)\n",
    "   - `FAST_MODE = True` → Uses 1 epoch for quick testing\n",
    "   - `RUN_FAIRNESS = True` → Computes fairness metrics on Jigsaw dataset\n",
    "\n",
    "### Expected Outputs\n",
    "- `experiments/summary_*.csv` - Performance metrics for each model/dataset\n",
    "- `experiments/preds_*.csv` - Predictions for fairness analysis\n",
    "- `experiments/fairness_*_summary.csv` - Fairness gap metrics\n",
    "- `experiments/*.png` - Visualization plots\n",
    "- `experiments/*.tex` - LaTeX tables for report\n",
    "\n",
    "### Key Findings to Report\n",
    "1. **In-Domain Performance**: How well models perform on their training distribution\n",
    "2. **Cross-Domain Generalization**: Performance drop when tested on different datasets\n",
    "3. **Fairness**: Demographic parity and equal opportunity gaps across identity groups"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
