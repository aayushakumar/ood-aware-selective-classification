{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Cell - Add this at the top of each notebook\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Set base directories based on environment\n",
    "if IS_KAGGLE:\n",
    "    INPUT_ROOT = \"/kaggle/input\"\n",
    "    WORK_DIR = \"/kaggle/working\"\n",
    "elif IS_COLAB:\n",
    "    INPUT_ROOT = \"/content/input\"\n",
    "    WORK_DIR = \"/content/working\"\n",
    "else:\n",
    "    # Local environment\n",
    "    INPUT_ROOT = Path.cwd() / \"input\"\n",
    "    WORK_DIR = Path.cwd() / \"working\"\n",
    "\n",
    "# Create standard directories\n",
    "OUT_DIR = os.path.join(WORK_DIR, \"data\")\n",
    "EXPERIMENTS_DIR = os.path.join(WORK_DIR, \"experiments\")\n",
    "SCRIPTS_DIR = os.path.join(WORK_DIR, \"scripts\")\n",
    "\n",
    "# Create all directories\n",
    "for directory in [OUT_DIR, EXPERIMENTS_DIR, SCRIPTS_DIR]:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "print(f\"Input directory: {INPUT_ROOT}\")\n",
    "print(f\"Working directory: {WORK_DIR}\")\n",
    "print(f\"Data directory: {OUT_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Experiment Runner for CS483 BiasBreakers Project\n",
    "# This notebook orchestrates all experiments and generates results for the final report\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"/kaggle/working/data\"\n",
    "EXPERIMENTS_DIR = \"/kaggle/working/experiments\"\n",
    "SCRIPTS_DIR = \"/kaggle/working/scripts\"\n",
    "\n",
    "# Create directories\n",
    "Path(DATA_DIR).mkdir(exist_ok=True)\n",
    "Path(EXPERIMENTS_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BiasBreakers: Master Experiment Runner\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nData directory: {DATA_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")\n",
    "print(f\"Scripts directory: {SCRIPTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Verify data preprocessing is complete\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Verifying Data Files\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "required_files = [\n",
    "    \"jigsaw_train.csv\", \"jigsaw_val.csv\", \"jigsaw_test.csv\",\n",
    "    \"jigsaw_train_full.csv\", \"jigsaw_val_full.csv\", \"jigsaw_test_full.csv\",\n",
    "    \"civil_train.csv\", \"civil_val.csv\", \"civil_test.csv\",\n",
    "    \"civil_train_full.csv\", \"civil_val_full.csv\", \"civil_test_full.csv\",\n",
    "    \"hatexplain_train.csv\", \"hatexplain_val.csv\", \"hatexplain_test.csv\",\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for f in required_files:\n",
    "    path = Path(DATA_DIR) / f\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024*1024)\n",
    "        print(f\"‚úì {f} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚úó {f} - MISSING\")\n",
    "        missing_files.append(f)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {len(missing_files)} files missing!\")\n",
    "    print(\"Please run preprocessing notebooks first:\")\n",
    "    print(\"  - cs483data.ipynb\")\n",
    "    print(\"  - civildata.ipynb\")\n",
    "    print(\"  - hatexplaindata.ipynb\")\n",
    "else:\n",
    "    print(f\"\\n‚úì All {len(required_files)} data files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Run TF-IDF Baselines\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Training TF-IDF Baselines\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# FIXED: Add scripts directory to path\n",
    "sys.path.insert(0, SCRIPTS_DIR)\n",
    "\n",
    "# Verify script exists before importing\n",
    "tfidf_script = Path(SCRIPTS_DIR) / \"run_tfidf_baselines.py\"\n",
    "if not tfidf_script.exists():\n",
    "    print(f\"‚ö†Ô∏è  ERROR: Script not found: {tfidf_script}\")\n",
    "    print(\"Please ensure run_tfidf_baselines.py is in the scripts directory\")\n",
    "    raise FileNotFoundError(f\"Missing script: {tfidf_script}\")\n",
    "\n",
    "# Import and run TF-IDF baseline\n",
    "try:\n",
    "    from run_tfidf_baselines import train_and_evaluate_tfidf\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Import Error: {e}\")\n",
    "    print(\"Trying subprocess approach instead...\")\n",
    "    \n",
    "    # Fallback: Run as subprocess\n",
    "    import subprocess\n",
    "    result = subprocess.run([\n",
    "        sys.executable,\n",
    "        str(tfidf_script),\n",
    "        \"--source_dataset\", \"jigsaw\",\n",
    "        \"--target_datasets\", \"civil\", \"hatexplain\",\n",
    "        \"--model\", \"logreg\",\n",
    "        \"--seed\", \"42\",\n",
    "        \"--data_dir\", DATA_DIR,\n",
    "        \"--save_preds\"\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "    \n",
    "    # Skip the rest if using subprocess\n",
    "    results_tfidf = None\n",
    "else:\n",
    "    # Experiment 2.1: Jigsaw ‚Üí Civil & HateXplain\n",
    "    print(\"Running: TF-IDF Logistic Regression (Jigsaw ‚Üí Civil, HateXplain)\")\n",
    "    results_tfidf = train_and_evaluate_tfidf(\n",
    "        source_dataset=\"jigsaw\",\n",
    "        target_datasets=[\"civil\", \"hatexplain\"],\n",
    "        model_type=\"logreg\",\n",
    "        seed=42,\n",
    "        data_dir=DATA_DIR,\n",
    "        save_preds=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì TF-IDF baseline complete!\")\n",
    "    print(f\"In-domain test F1: {results_tfidf['in_domain_test']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Run RoBERTa Models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Training RoBERTa Models\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from run_roberta import train_and_evaluate\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Experiment 3.1: Basic RoBERTa with calibration\n",
    "print(\"\\n--- Experiment 3.1: RoBERTa with Isotonic Calibration ---\")\n",
    "results_roberta = train_and_evaluate(\n",
    "    source_dataset=\"jigsaw\",\n",
    "    target_datasets=[\"civil\", \"hatexplain\"],\n",
    "    model_name=\"roberta-base\",\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    lr=2e-5,\n",
    "    max_len=128,\n",
    "    seed=42,\n",
    "    data_dir=DATA_DIR,\n",
    "    calibration=\"isotonic\",\n",
    "    early_stop=True,\n",
    "    patience=2,\n",
    "    tune_threshold=True,\n",
    "    save_preds=True,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì RoBERTa training complete!\")\n",
    "print(f\"In-domain test F1: {results_roberta['in_domain']['test']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b72725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Compute Fairness Metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Computing Fairness Metrics\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from scripts.fairness_metrics import compute_group_fairness\n",
    "import pandas as pd\n",
    "\n",
    "# Experiment 4.1: Cross-domain fairness (Jigsaw ‚Üí Civil)\n",
    "print(\"Computing fairness for: Jigsaw ‚Üí Civil\")\n",
    "\n",
    "# Load predictions and full data\n",
    "pred_file = Path(EXPERIMENTS_DIR) / \"preds_jigsaw_to_civil.csv\"\n",
    "full_data_file = Path(DATA_DIR) / \"civil_test_full.csv\"\n",
    "\n",
    "if pred_file.exists() and full_data_file.exists():\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    full_df = pd.read_csv(full_data_file)\n",
    "    \n",
    "    # Merge on ID\n",
    "    merged_df = pred_df.merge(full_df, on=\"id\", how=\"inner\", suffixes=(\"\", \"_full\"))\n",
    "    if \"label_full\" in merged_df.columns:\n",
    "        merged_df = merged_df.drop(columns=[\"label_full\"])\n",
    "    \n",
    "    # Find group columns\n",
    "    group_cols = [c for c in merged_df.columns if c.startswith(\"g_\")]\n",
    "    \n",
    "    print(f\"Found {len(group_cols)} identity groups\")\n",
    "    print(f\"Analyzing {len(merged_df)} predictions\")\n",
    "    \n",
    "    # Compute fairness\n",
    "    summary_df, per_group_df = compute_group_fairness(\n",
    "        merged_df,\n",
    "        group_cols=group_cols,\n",
    "        label_col=\"label\",\n",
    "        pred_col=\"pred\",\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    summary_df.to_csv(Path(EXPERIMENTS_DIR) / \"fairness_jigsaw_to_civil_summary.csv\", index=False)\n",
    "    per_group_df.to_csv(Path(EXPERIMENTS_DIR) / \"fairness_jigsaw_to_civil_per_group.csv\", index=False)\n",
    "    \n",
    "    # Print top fairness violations\n",
    "    print(\"\\nTop 5 groups by Demographic Parity difference:\")\n",
    "    print(summary_df.nlargest(5, \"dp_diff\")[[\"group_col\", \"dp_diff\", \"eop_diff\", \"eo_diff\"]])\n",
    "    \n",
    "    print(\"\\n‚úì Fairness analysis complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Required files not found. Ensure predictions and full data exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82766dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Generate Summary Statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: Summary Statistics\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Load all summary CSVs\n",
    "summary_files = list(Path(EXPERIMENTS_DIR).glob(\"summary_*.csv\"))\n",
    "print(f\"Found {len(summary_files)} summary files:\\n\")\n",
    "\n",
    "all_summaries = {}\n",
    "for f in summary_files:\n",
    "    df = pd.read_csv(f)\n",
    "    model_name = f.stem.replace(\"summary_\", \"\")\n",
    "    all_summaries[model_name] = df\n",
    "    \n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(df[[\"split\", \"accuracy\", \"f1\", \"auroc\", \"pr_auc\"]].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "# Create comparison table\n",
    "comparison_rows = []\n",
    "for model_name, df in all_summaries.items():\n",
    "    for _, row in df.iterrows():\n",
    "        comparison_rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Split\": row[\"split\"],\n",
    "            \"Accuracy\": f\"{row.get('accuracy', 0):.4f}\",\n",
    "            \"F1\": f\"{row.get('f1', 0):.4f}\",\n",
    "            \"AUROC\": f\"{row.get('auroc', 0):.4f}\",\n",
    "            \"PR-AUC\": f\"{row.get('pr_auc', 0):.4f}\",\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "comparison_df.to_csv(Path(EXPERIMENTS_DIR) / \"model_comparison.csv\", index=False)\n",
    "print(\"‚úì Model comparison saved to: model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623503f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Quick Visualization Preview\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: Quick Visualization Preview\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot 1: Cross-domain performance comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "models_to_plot = [\"jigsaw\", \"tfidf_jigsaw_logreg\"]\n",
    "colors = [\"steelblue\", \"coral\"]\n",
    "\n",
    "for idx, (model_name, color) in enumerate(zip(models_to_plot, colors)):\n",
    "    if model_name in all_summaries:\n",
    "        df = all_summaries[model_name]\n",
    "        splits = df[\"split\"].values\n",
    "        f1_scores = df[\"f1\"].values\n",
    "        \n",
    "        x_pos = range(len(splits))\n",
    "        x_pos = [x + idx*0.35 for x in x_pos]\n",
    "        \n",
    "        ax.bar(x_pos, f1_scores, width=0.35, label=model_name, color=color, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel(\"Dataset Split\", fontsize=12)\n",
    "ax.set_ylabel(\"F1 Score\", fontsize=12)\n",
    "ax.set_title(\"Cross-Domain Performance Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks([r + 0.175 for r in range(len(splits))])\n",
    "ax.set_xticklabels(splits, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(EXPERIMENTS_DIR) / \"plots\" / \"quick_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Quick visualization complete!\")\n",
    "print(\"For full analysis, run: scripts/analysis_plots.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL: Experiment Summary Report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY REPORT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üìä COMPLETED EXPERIMENTS:\")\n",
    "print(\"  ‚úì TF-IDF Baseline (Logistic Regression)\")\n",
    "print(\"  ‚úì RoBERTa with Calibration\")\n",
    "print(\"  ‚úì Cross-Domain Evaluation (Civil, HateXplain)\")\n",
    "print(\"  ‚úì Fairness Analysis\")\n",
    "print(\"  ‚úì Summary Statistics\\n\")\n",
    "\n",
    "print(\"üìÅ OUTPUT FILES:\")\n",
    "output_files = list(Path(EXPERIMENTS_DIR).glob(\"*\"))\n",
    "print(f\"  Total files generated: {len(output_files)}\")\n",
    "print(f\"  Summary CSVs: {len(list(Path(EXPERIMENTS_DIR).glob('summary_*.csv')))}\")\n",
    "print(f\"  Prediction CSVs: {len(list(Path(EXPERIMENTS_DIR).glob('preds_*.csv')))}\")\n",
    "print(f\"  Fairness CSVs: {len(list(Path(EXPERIMENTS_DIR).glob('fairness_*.csv')))}\")\n",
    "\n",
    "print(\"\\nüìà NEXT STEPS:\")\n",
    "print(\"  1. Run scripts/analysis_plots.ipynb to generate all visualizations\")\n",
    "print(\"  2. Review fairness metrics in fairness_*_summary.csv\")\n",
    "print(\"  3. Copy key plots from experiments/plots/ to your report\")\n",
    "print(\"  4. Use model_comparison.csv for quantitative results table\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All experiments complete! Ready for report generation.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259be9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
