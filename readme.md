# BiasBreakers: Cross-Domain Toxicity Detection with Fairness Analysis

CS 483 Course Project - Out-of-Distribution Evaluation of Toxic Classifiers

## Overview

This project implements and evaluates toxicity detection models across multiple domains, with a focus on:
- **Cross-domain generalization** (train on Jigsaw, test on Civil Comments & HateXplain)
- **Model calibration** (temperature scaling, isotonic regression)
- **Fairness analysis** (demographic parity, equal opportunity, equalized odds)
- **Classical baselines** (TF-IDF + Logistic Regression/SVM)
- **Deep learning models** (RoBERTa with optional LoRA and CORAL domain adaptation)

## Repository Structure

```
ood-eval-toxic-classifiers-Han/
├── scripts/
│   ├── cs483data.ipynb           # Jigsaw data preprocessing
│   ├── civildata.ipynb           # Civil Comments preprocessing
│   ├── hatexplaindata.ipynb      # HateXplain preprocessing
│   ├── run_roberta.py            # RoBERTa training & evaluation
│   ├── run_tfidf_baselines.py    # TF-IDF baseline models
│   ├── fairness_metrics.py       # Fairness analysis script
│   └── analysis_plots.ipynb      # Visualization & reporting
├── data/                         # Generated by preprocessing notebooks
│   ├── jigsaw_train.csv         # Training data (text, label)
│   ├── jigsaw_train_full.csv    # With id & group attributes
│   ├── civil_*.csv              # Civil Comments splits
│   ├── hatexplain_*.csv         # HateXplain splits
│   └── *_protocols.json         # Dataset statistics
├── experiments/                  # Generated by training scripts
│   ├── summary_*.csv            # Aggregated metrics
│   ├── preds_*.csv              # Prediction files
│   ├── fairness_*.csv           # Fairness analysis results
│   └── plots/                   # Generated visualizations
└── README.md
```

## Setup & Installation

### On Kaggle (Recommended)

1. **Add Required Datasets** to your Kaggle notebook:
   - Jigsaw Unintended Bias in Toxicity Classification
   - Civil Comments (if available)
   - HateXplain dataset

2. **Install Dependencies** (most are pre-installed on Kaggle):
   ```python
   !pip install transformers scikit-learn pandas numpy matplotlib seaborn
   # Optional for LoRA:
   !pip install peft
   ```

3. **Enable GPU** in notebook settings for faster training

### Local Setup

```bash
pip install torch transformers scikit-learn pandas numpy matplotlib seaborn
pip install peft  # Optional, for LoRA
```

## Experiment Pipeline

### Step 1: Data Preprocessing

Run these notebooks **in order** on Kaggle to prepare datasets:

#### 1a. Jigsaw Dataset
Open and run `scripts/cs483data.ipynb`:
- Loads Jigsaw toxic comments
- Cleans text (URL/handle normalization)
- Binarizes toxicity at 0.5 threshold
- Creates stratified 8/1/1 train/val/test splits
- Exports both standard CSVs (for training) and full CSVs (with identity attributes for fairness)

**Outputs:**
- `data/jigsaw_train.csv`, `jigsaw_val.csv`, `jigsaw_test.csv`
- `data/jigsaw_train_full.csv`, `jigsaw_val_full.csv`, `jigsaw_test_full.csv`
- `data/protocols.json`

#### 1b. Civil Comments Dataset
Open and run `scripts/civildata.ipynb`:
- Loads Civil Comments
- Applies same preprocessing pipeline as Jigsaw
- Extracts identity group attributes
- Creates splits and exports CSVs

**Outputs:**
- `data/civil_train.csv`, `civil_val.csv`, `civil_test.csv`
- `data/civil_train_full.csv`, `civil_val_full.csv`, `civil_test_full.csv`
- `data/civil_protocols.json`

#### 1c. HateXplain Dataset
Open and run `scripts/hatexplaindata.ipynb`:
- Loads HateXplain (JSON/JSONL format)
- Maps labels to binary (hatespeech/offensive → 1, else → 0)
- Creates splits and exports CSVs

**Outputs:**
- `data/hatexplain_train.csv`, `hatexplain_val.csv`, `hatexplain_test.csv`
- `data/hatexplain_protocols.json`

### Step 2: Train Classical Baselines

Run TF-IDF + Logistic Regression baseline:

```bash
python scripts/run_tfidf_baselines.py \
  --source_dataset jigsaw \
  --target_datasets civil hatexplain \
  --model logreg \
  --seed 42 \
  --save_preds
```

**Key Arguments:**
- `--source_dataset`: Training dataset (jigsaw, civil, hatexplain)
- `--target_datasets`: Cross-domain evaluation targets
- `--model`: Model type (logreg, svm, both)
- `--ngram_max`: Maximum n-gram (default: 2)
- `--min_df`: Minimum document frequency (default: 5)
- `--seeds`: Multiple seeds for robust evaluation

**Outputs:**
- `experiments/summary_tfidf_jigsaw_logreg.csv`
- `experiments/preds_tfidf_logreg_jigsaw_to_civil.csv`

### Step 3: Train RoBERTa Models

#### 3a. Basic RoBERTa (no domain adaptation)

```bash
python scripts/run_roberta.py \
  --source_dataset jigsaw \
  --target_datasets civil hatexplain \
  --model_name roberta-base \
  --epochs 3 \
  --batch_size 16 \
  --lr 2e-5 \
  --max_len 128 \
  --seed 42 \
  --calibration isotonic \
  --early_stop \
  --tune_threshold \
  --save_preds
```

#### 3b. RoBERTa with LoRA (Parameter-Efficient Fine-Tuning)

```bash
python scripts/run_roberta.py \
  --source_dataset jigsaw \
  --target_datasets civil hatexplain \
  --peft lora \
  --lora_r 8 \
  --lora_alpha 16 \
  --epochs 5 \
  --calibration temperature \
  --save_preds
```

#### 3c. RoBERTa with CORAL Domain Adaptation

```bash
python scripts/run_roberta.py \
  --source_dataset jigsaw \
  --target_datasets civil hatexplain \
  --coral_target civil \
  --coral_lambda 0.1 \
  --epochs 5 \
  --save_preds
```

#### 3d. Multi-seed Robust Evaluation

```bash
python scripts/run_roberta.py \
  --source_dataset jigsaw \
  --target_datasets civil hatexplain \
  --seeds 42 123 456 \
  --epochs 3 \
  --calibration isotonic \
  --save_preds
```

**Key Arguments:**
- `--calibration`: Post-hoc calibration (none, temperature, isotonic)
- `--peft`: Parameter-efficient fine-tuning (none, lora)
- `--coral_target`, `--coral_lambda`: CORAL domain adaptation
- `--amp`: Mixed precision training (faster on GPU)
- `--weighted_sampler`: Handle class imbalance
- `--early_stop`, `--patience`: Early stopping
- `--tune_threshold`: Optimize decision threshold on validation

**Outputs:**
- `experiments/summary_jigsaw.csv`
- `experiments/preds_jigsaw_test.csv`
- `experiments/preds_jigsaw_to_civil.csv`
- `experiments/jigsaw_val_reliability.csv` (calibration bins)

### Step 4: Fairness Analysis

Compute fairness metrics for predictions with identity attributes:

```bash
python scripts/fairness_metrics.py \
  --dataset civil \
  --split test \
  --pred_file experiments/preds_jigsaw_to_civil.csv \
  --full_data_file data/civil_test_full.csv \
  --out_prefix experiments/fairness_jigsaw_to_civil
```

**Outputs:**
- `experiments/fairness_jigsaw_to_civil_summary.csv` (DP/EO/EOdds gaps per group)
- `experiments/fairness_jigsaw_to_civil_per_group.csv` (TPR/FPR per group)

**Fairness Metrics Computed:**
- **Demographic Parity (DP)**: Difference in positive prediction rates
- **Equal Opportunity (EO)**: Difference in TPR across groups
- **Equalized Odds (EOdds)**: Max difference in TPR and FPR

### Step 5: Generate Plots & Analysis

Open and run `scripts/analysis_plots.ipynb` to generate:

1. **Reliability Diagrams**: Calibration quality visualization
2. **ROC Curves**: Discrimination performance
3. **Precision-Recall Curves**: Performance on imbalanced data
4. **Confusion Matrices**: Error analysis
5. **Cross-Domain Comparison**: Model comparison across domains
6. **Fairness Visualizations**: Group-wise bias analysis

**All plots saved to:** `experiments/plots/`

## Recommended Experiment Workflow for Report

For a complete evaluation covering all project requirements:

### Experiment 1: In-Domain Performance
```bash
# TF-IDF baseline
python scripts/run_tfidf_baselines.py --source_dataset jigsaw --model logreg --save_preds

# RoBERTa
python scripts/run_roberta.py --source_dataset jigsaw --epochs 3 --save_preds --calibration isotonic
```

### Experiment 2: Cross-Domain Generalization
```bash
# Train on Jigsaw, test on Civil & HateXplain
python scripts/run_roberta.py \
  --source_dataset jigsaw \
  --target_datasets civil hatexplain \
  --epochs 3 \
  --calibration isotonic \
  --tune_threshold \
  --save_preds
```

### Experiment 3: Domain Adaptation (CORAL)
```bash
python scripts/run_roberta.py \
  --source_dataset jigsaw \
  --target_datasets civil \
  --coral_target civil \
  --coral_lambda 0.1 \
  --epochs 5 \
  --save_preds
```

### Experiment 4: Fairness Analysis
```bash
# In-domain fairness (train & test on Civil)
python scripts/run_roberta.py --source_dataset civil --epochs 3 --save_preds
python scripts/fairness_metrics.py \
  --dataset civil --split test \
  --pred_file experiments/preds_civil_test.csv \
  --full_data_file data/civil_test_full.csv \
  --out_prefix experiments/fairness_civil_indomain

# Cross-domain fairness (train on Jigsaw, test on Civil)
python scripts/fairness_metrics.py \
  --dataset civil --split test \
  --pred_file experiments/preds_jigsaw_to_civil.csv \
  --full_data_file data/civil_test_full.csv \
  --out_prefix experiments/fairness_jigsaw_to_civil
```

### Experiment 5: Generate All Plots
Run `scripts/analysis_plots.ipynb` to create figures for report.

## Key Features Implemented

### Algorithms
✅ **Algorithm 1**: RoBERTa-base fine-tuned classifier  
✅ **Algorithm 2**: TF-IDF + Logistic Regression  
✅ **Algorithm 3**: TF-IDF + Linear SVM  
✅ **Algorithm 4**: CORAL domain adaptation (optional)  
✅ **Algorithm 5**: LoRA parameter-efficient fine-tuning (optional)

### Evaluation Methods
✅ Standard metrics: Accuracy, F1, Macro-F1, AUROC, PR-AUC  
✅ Per-class metrics: Precision, Recall, F1 for each class  
✅ Calibration: ECE, reliability diagrams, temperature scaling, isotonic regression  
✅ Cross-domain evaluation: Multiple target datasets  
✅ Fairness metrics: DP, Equal Opportunity, Equalized Odds  
✅ Confusion matrices and error analysis

### Advanced Features
✅ Multi-seed evaluation for robustness  
✅ Early stopping with validation monitoring  
✅ Mixed precision training (AMP) for speed  
✅ Weighted sampling for class imbalance  
✅ Threshold tuning for optimal F1  
✅ Prediction saving with IDs for fairness analysis

## Metrics Summary

All metrics are saved in CSV format in `experiments/`:

| Metric | Description |
|--------|-------------|
| accuracy | Overall classification accuracy |
| f1 | Binary F1 score (class 1) |
| f1_macro | Macro-averaged F1 |
| auroc | Area under ROC curve |
| pr_auc | Area under Precision-Recall curve |
| ece | Expected Calibration Error |
| brier | Brier score (calibration) |
| nll | Negative log-likelihood |
| dp_diff | Demographic Parity difference (fairness) |
| eop_diff | Equal Opportunity difference (fairness) |
| eo_diff | Equalized Odds difference (fairness) |

## Tips for Kaggle Execution

1. **Data Persistence**: After running preprocessing notebooks, download the `data/` folder and re-upload as a Kaggle dataset for reuse.

2. **GPU Usage**: Enable GPU for RoBERTa training (Settings → Accelerator → GPU).

3. **Memory Management**: 
   - Use `batch_size=8` or `16` for RoBERTa
   - Use `max_len=128` or `256` depending on GPU memory

4. **Time Limits**: Kaggle notebooks have execution time limits. For multi-seed runs:
   - Run 1-2 seeds per notebook session
   - Save checkpoints frequently

5. **Importing Scripts**: In notebooks, you can import functions:
   ```python
   import sys
   sys.path.append('/kaggle/working/scripts')
   from run_roberta import evaluate, load_dataset
   ```

## Expected Results

Based on similar studies, you should expect:

| Model | Jigsaw Test F1 | Civil Test F1 | HateXplain F1 |
|-------|---------------|---------------|---------------|
| TF-IDF LR | 0.70-0.75 | 0.60-0.65 | 0.55-0.62 |
| RoBERTa | 0.75-0.82 | 0.65-0.72 | 0.62-0.70 |
| RoBERTa + Calibration | Similar F1, Lower ECE | | |
| RoBERTa + CORAL | Slight drop in-domain | +2-5% cross-domain | |

**Fairness**: Expect DP/EO gaps of 0.05-0.15 for identity groups like gender and race.

## Troubleshooting

### Data Loading Issues
- Ensure CSV files have correct column names: `text`, `label`, optionally `id`
- Check that `_full.csv` files include `g_*` columns for fairness

### Training Issues
- **OOM Error**: Reduce `batch_size` or `max_len`
- **Slow Training**: Enable `--amp` for mixed precision
- **Poor Convergence**: Increase `--epochs` or adjust `--lr`

### Fairness Analysis Issues
- Ensure prediction CSV and full data CSV have matching `id` columns
- Check that group columns exist in `_full.csv` files

## Citation

If you use this code, please cite:

```
@misc{biasbreakers2025,
  title={BiasBreakers: Cross-Domain Toxicity Detection with Fairness Analysis},
  author={Your Name},
  year={2025},
  note={CS 483 Course Project}
}
```

## References

- Jigsaw Toxic Comment Dataset: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification
- Civil Comments: https://arxiv.org/abs/1903.04561
- HateXplain: https://arxiv.org/abs/2012.10289
- RoBERTa: https://arxiv.org/abs/1907.11692
- CORAL: https://arxiv.org/abs/1612.01939
- LoRA: https://arxiv.org/abs/2106.09685

## License

This project is for educational purposes (CS 483 course project).